{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def impute (datasets: list[pd.DataFrame]) : \n",
    "\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "\n",
    "    imputed_sets = []\n",
    "\n",
    "    for set in datasets: \n",
    "\n",
    "        # storing columns to lable after impute, also removing date column as this does not work with impute \n",
    "        cols = set.columns \n",
    "\n",
    "        if set.columns.__contains__(\"date_forecast\"): \n",
    "            dates = set[\"date_forecast\"]\n",
    "        \n",
    "        if set.columns.__contains__(\"date_calc\"): \n",
    "            set = set.drop(\"date_calc\", axis=1)\n",
    "\n",
    "        cols = set.columns.delete(0)\n",
    "\n",
    "        set_wo_date = set.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "        #imputing (estimating) missing values \n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\", add_indicator=False)\n",
    "        imp.fit(set_wo_date)\n",
    "        set_wo_date = imp.transform(set_wo_date)\n",
    "        set_wo_date = pd.DataFrame(set_wo_date)\n",
    "\n",
    "        # setting column lables basck\n",
    "        set = set_wo_date\n",
    "        set.columns = set.columns.astype(str)\n",
    "        set.columns = cols\n",
    "        set[\"date_forecast\"] = dates\n",
    "\n",
    "        #sorting columns \n",
    "        cols = cols.tolist()\n",
    "        cols.insert(0, \"date_forecast\")\n",
    "\n",
    "        set = set[cols]\n",
    "\n",
    "        set = set.fillna(0.0)\n",
    "\n",
    "        imputed_sets.append(set)\n",
    "\n",
    "    return imputed_sets\n",
    "\n",
    "\n",
    "def drop_feature(datasets:list[pd.DataFrame], features:list[str]):\n",
    "\n",
    "    altered_sets = []\n",
    "\n",
    "    for set in datasets: \n",
    "        for feature in features:\n",
    "\n",
    "            set = set.drop(feature, axis=1)\n",
    "\n",
    "        altered_sets.append(set)\n",
    "\n",
    "    return altered_sets\n",
    "\n",
    "def find_const_interval(df, target_attribute, interval_length, ignore_values=[]):\n",
    "    \"\"\"\n",
    "    Find all the intervals of the given length in the dataset where the target_attribute is constant.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe.\n",
    "    target_attribute (str): The target attribute to check for constant values.\n",
    "    interval_length (int): The length of the interval to search for.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of indexes of all the values in the intervals where the target_attribute is constant for the given interval length or bigger.\n",
    "    \"\"\"\n",
    "    idxs = []\n",
    "    intervals_found = 0\n",
    "    i = 0\n",
    "    while i + interval_length < len(df) - 1:\n",
    "        if df[target_attribute][i:i+interval_length].nunique() == 1 and df[target_attribute][i] not in ignore_values:\n",
    "            j = i + interval_length - 1\n",
    "            value = df[target_attribute][i]\n",
    "            while value == df[target_attribute][j] and j < len(df) - 1:\n",
    "                j += 1\n",
    "            idxs.extend(list(range(i, j)))\n",
    "            i = j+2\n",
    "            intervals_found += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return idxs, intervals_found\n",
    "\n",
    "def donate_missing_rows(reciever, donor, target_attribute = 'date_forecast', scalepv = False):\n",
    "    \"\"\"\n",
    "    Add rows from donor to reciever where the target_attribute is missing in reciever.\n",
    "\n",
    "    Parameters:\n",
    "    reciever (pandas.DataFrame): The dataframe to add rows to.\n",
    "    donor (pandas.DataFrame): The dataframe to add rows from.\n",
    "    target_attribute (str): The target attribute to check for missing values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the updated reciever dataframe and the number of rows that were donated.\n",
    "    \"\"\"\n",
    "    donor_copy = donor.copy()\n",
    "    if scalepv:\n",
    "        scale = reciever['pv_measurement'].mean() / donor['pv_measurement'].mean()\n",
    "        donor_copy['pv_measurement'] = donor_copy['pv_measurement'] * scale\n",
    "\n",
    "    donated_rows = 0\n",
    "    for date in donor_copy[target_attribute]:\n",
    "        if date not in reciever[target_attribute].values:\n",
    "            new_row = donor_copy[donor_copy[target_attribute] == date]\n",
    "            reciever = pd.concat([reciever, new_row], ignore_index=True)\n",
    "            donated_rows += 1\n",
    "    reciever = reciever.sort_values(target_attribute).reset_index(drop=True)\n",
    "\n",
    "    return reciever, donated_rows\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Datamanager \n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class Data_Manager() : \n",
    "\n",
    "    def __init__(self) : \n",
    "        # Y_train\n",
    "        self.train_a = pd.DataFrame() \n",
    "        self.train_b = pd.DataFrame()\n",
    "        self.train_c = pd.DataFrame()\n",
    "\n",
    "        self.X_train_observed_a = pd.DataFrame()\n",
    "        self.X_train_observed_b = pd.DataFrame()\n",
    "        self.X_train_observed_c = pd.DataFrame()\n",
    "\n",
    "        self.X_train_estimated_a = pd.DataFrame()\n",
    "        self.X_train_estimated_b = pd.DataFrame()\n",
    "        self.X_train_estimated_c = pd.DataFrame()\n",
    "\n",
    "        self.X_test_estimated_a = pd.DataFrame()\n",
    "        self.X_test_estimated_b = pd.DataFrame()\n",
    "        self.X_test_estimated_c = pd.DataFrame()\n",
    "\n",
    "        self.data_A = pd.DataFrame()    \n",
    "        self.data_B = pd.DataFrame()\n",
    "        self.data_C = pd.DataFrame()\n",
    "\n",
    "        self.data = pd.DataFrame()\n",
    "        self.X_test_estimated = pd.DataFrame()\n",
    "\n",
    "        # X_train_obs, Y_train_obs\n",
    "        self.data_A_obs = pd.DataFrame()    \n",
    "        self.data_B_obs = pd.DataFrame()\n",
    "        self.data_C_obs = pd.DataFrame()\n",
    "        \n",
    "        # X_train_obs, Y_train_obs\n",
    "        self.data_A_es = pd.DataFrame()\n",
    "        self.data_B_es = pd.DataFrame()\n",
    "        self.data_C_es = pd.DataFrame()\n",
    "\n",
    "        self.amplitude = np.zeros(3) # amp_a, amp_b, amp_c\n",
    "\n",
    "    def data_loader(self): \n",
    "        \"\"\"\n",
    "        Function that loads the datasets into data manager, loads all data \n",
    "        \"\"\"\n",
    "\n",
    "        self.train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "        self.train_a = self.train_a.rename(columns={\"time\":\"date_forecast\"})\n",
    "\n",
    "        self.train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "        self.train_b = self.train_b.rename(columns={\"time\":\"date_forecast\"})\n",
    "\n",
    "        self.train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "        self.train_c = self.train_c.rename(columns={\"time\":\"date_forecast\"})\n",
    "\n",
    "        self.X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "        self.X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "        self.X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "        self.X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "        self.X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "        self.X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "        self.X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "        self.X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "        self.X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n",
    "    \n",
    "    def dms2dm(self, dms):\n",
    "        self.train_a = dms.data['train_a']\n",
    "        self.train_b = dms.data['train_b']\n",
    "        self.train_c = dms.data['train_c']\n",
    "\n",
    "        self.X_train_estimated_a = dms.data['X_train_estimated_a']\n",
    "        self.X_train_estimated_b = dms.data['X_train_estimated_b']\n",
    "        self.X_train_estimated_c = dms.data['X_train_estimated_c']\n",
    "\n",
    "        self.X_train_observed_a = dms.data['X_train_observed_a']\n",
    "        self.X_train_observed_b = dms.data['X_train_observed_b']\n",
    "        self.X_train_observed_c = dms.data['X_train_observed_c']\n",
    "\n",
    "        self.X_test_estimated_a = dms.data['X_test_estimated_a']\n",
    "        self.X_test_estimated_b = dms.data['X_test_estimated_b']\n",
    "        self.X_test_estimated_c = dms.data['X_test_estimated_c']\n",
    "\n",
    "        self.data_A_obs = dms.data['data_A_obs']\n",
    "        self.data_B_obs = dms.data['data_B_obs']\n",
    "        self.data_C_obs = dms.data['data_C_obs']\n",
    "\n",
    "        self.data_A_es = dms.data['data_A_es']\n",
    "        self.data_B_es = dms.data['data_B_es']\n",
    "        self.data_C_es = dms.data['data_C_es']\n",
    "\n",
    "        self.data_A = dms.data['data_A']   \n",
    "        self.data_B = dms.data['data_B']\n",
    "        self.data_C = dms.data['data_C']\n",
    "\n",
    "        self.amplitude = dms.data['amplitude']\n",
    "\n",
    "    def drop_feature(self, datasets:list[pd.DataFrame], features:list[str]):\n",
    "        \"\"\"\n",
    "        Takes in list of datasets and removes features from the sets\n",
    "\n",
    "        Returns list of altered datasets\n",
    "        \"\"\"\n",
    "\n",
    "        altered_sets = []\n",
    "\n",
    "        for set in datasets: \n",
    "            for feature in features:\n",
    "\n",
    "                set = set.drop(feature, axis=1)\n",
    "\n",
    "            altered_sets.append(set)\n",
    "\n",
    "        return altered_sets\n",
    "    \n",
    "    def combine_data(self): \n",
    "        import pandas as pd\n",
    "        \"\"\"\n",
    "        Combines datasets A, B and C into one set containing all features and pv_measurements. \n",
    "\n",
    "        Combine_observed_estimated (bool) determines if you want one single set for A B C or keep the observed and estimated\n",
    "        data split \n",
    "\n",
    "        Warning! Data should have no NaN values or be of same frequency before combining! \n",
    "        \"\"\"\n",
    "        weather_data_A = pd.concat([self.X_train_observed_a, self.X_train_estimated_a], axis=0, ignore_index=True)\n",
    "        weather_data_B = pd.concat([self.X_train_observed_b, self.X_train_estimated_b], axis=0, ignore_index=True)\n",
    "        weather_data_C = pd.concat([self.X_train_observed_c, self.X_train_estimated_c], axis=0, ignore_index=True)\n",
    "\n",
    "        self.data_A = pd.merge(weather_data_A, self.train_a, how=\"left\", on=\"date_forecast\")\n",
    "        self.data_B = pd.merge(weather_data_B, self.train_b,  on=\"date_forecast\", how=\"left\")\n",
    "        self.data_C = pd.merge(weather_data_C, self.train_c, on=\"date_forecast\", how=\"left\")\n",
    "\n",
    "        if ( self.data_A.columns.__contains__(\"date_calc\") ): \n",
    "            self.data_A = self.data_A.drop(\"date_calc\", axis=1)\n",
    "            self.data_B = self.data_B.drop(\"date_calc\", axis=1)\n",
    "            self.data_C = self.data_C.drop(\"date_calc\", axis=1)\n",
    "\n",
    "        if (self.train_a.shape[0] > 35000 ) : \n",
    "            \n",
    "            self.data_A = self.makima_interpolate(self.data_A).dropna()\n",
    "            self.data_B = self.makima_interpolate(self.data_B).dropna()\n",
    "            self.data_C = self.makima_interpolate(self.data_C).dropna()\n",
    "        \n",
    "        elif (self.train_a.shape[0] < 35000) : \n",
    "            self.data_A = self.data_A.dropna()\n",
    "            self.data_B = self.data_B.dropna()\n",
    "            self.data_C = self.data_C.dropna()\n",
    "\n",
    "        if self.data_A.isna().sum().sum() > 0 :\n",
    "            warnings.warn(\"Warning! Data should have no NaN values or be of same frequency before combining! Use impute or interpolation on data before combining! This could also come from dates in the combined datasets not overlapping fully.\")\n",
    "\n",
    "        return self.data_A, self.data_B, self.data_C\n",
    "\n",
    "    def impute_data(self, datasets, advanced_imputer=False):\n",
    "\n",
    "        \"\"\"\n",
    "        imputes data to fill in missing values\n",
    "\n",
    "        takes in a list of datasets\n",
    "\n",
    "        returns list of imputed data\n",
    "\n",
    "        removes all columns consisting entirely of nan \n",
    "        \"\"\"\n",
    "\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "        from tqdm import tqdm\n",
    "        imputed_sets = []\n",
    "\n",
    "        for set in tqdm(datasets): \n",
    "\n",
    "            # storing columns to lable after impute, also removing date column as this does not work with impute \n",
    "            cols = set.columns \n",
    "\n",
    "            if set.columns.__contains__(\"date_forecast\"): \n",
    "                dates = set[\"date_forecast\"]\n",
    "            \n",
    "            if set.columns.__contains__(\"date_calc\"): \n",
    "                set = set.drop(\"date_calc\", axis=1)\n",
    "\n",
    "            cols = set.columns.delete(0)\n",
    "\n",
    "            set_wo_date = set.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "            #imputing (estimating) missing values \n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\", add_indicator=False)\n",
    "            imp.fit(set_wo_date)\n",
    "            set_wo_date = pd.DataFrame(imp.transform(set_wo_date), columns=imp.get_feature_names_out())\n",
    "            \n",
    "\n",
    "            # setting column lables basck\n",
    "            set = set_wo_date\n",
    "            \n",
    "            set[\"date_forecast\"] = dates\n",
    "\n",
    "            #sorting columns \n",
    "            cols = cols.tolist()\n",
    "            cols.insert(0, \"date_forecast\")\n",
    "\n",
    "            #set = set.fillna(0.0)\n",
    "\n",
    "            imputed_sets.append(set)\n",
    "\n",
    "        return imputed_sets\n",
    "    \n",
    "    def iterative_imputer(self, datasets) :\n",
    "\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "        from tqdm import tqdm\n",
    "        imputed_sets = []\n",
    "        imp = IterativeImputer(random_state=0, missing_values=np.nan, add_indicator=False, imputation_order=\"ascending\", skip_complete=True)\n",
    "\n",
    "        for set in tqdm(datasets): \n",
    "\n",
    "            cols = set.columns \n",
    "\n",
    "            if set.columns.__contains__(\"date_forecast\"): \n",
    "                dates = set[\"date_forecast\"]\n",
    "            \n",
    "            if set.columns.__contains__(\"date_calc\"): \n",
    "                set = set.drop(\"date_calc\", axis=1)\n",
    "            \n",
    "            cols = set.columns.delete(0)\n",
    "\n",
    "            set_wo_date = set.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "            print(\"getting to imputing\")\n",
    "            #imputing (estimating) missing values \n",
    "            imp.fit(set_wo_date)\n",
    "            \n",
    "            set_wo_date = pd.DataFrame(imp.transform(set_wo_date), columns=imp.get_feature_names_out())\n",
    "\n",
    "             # setting column lables basck\n",
    "            set = set_wo_date\n",
    "            \n",
    "            set[\"date_forecast\"] = dates\n",
    "\n",
    "            # set = set.fillna(0.0)\n",
    "\n",
    "\n",
    "            #sorting columns \n",
    "            cols = cols.tolist()\n",
    "            cols.insert(0, \"date_forecast\")\n",
    "\n",
    "            imputed_sets.append(set)\n",
    "\n",
    "        return imputed_sets\n",
    "\n",
    "    def resample_data(self, datasets, freq) : \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        resamples the given dataset into the correct frequency. \n",
    "        H : hourly \n",
    "        15T : 15min \n",
    "        \"\"\"\n",
    "\n",
    "        corr = []\n",
    "\n",
    "        for set in datasets: \n",
    "            set_hourly = set.resample(freq, on=\"date_forecast\").mean()\n",
    "\n",
    "            set_dates = set[\"date_forecast\"].dt.date.unique().tolist()\n",
    "\n",
    "            set_hourly[\"date_forecast\"] = set_hourly.index\n",
    "\n",
    "            set_corr = set_hourly[set_hourly[\"date_forecast\"].dt.date.isin(set_dates)]\n",
    "\n",
    "            set_corr.index = pd.RangeIndex(0, len(set_corr))\n",
    "            corr.append(set_corr)\n",
    "\n",
    "        \n",
    "        return corr\n",
    "\n",
    "    def add_feature(dataset, feature_name, data) :\n",
    "\n",
    "        added_set = dataset[feature_name] = data\n",
    "\n",
    "        return added_set\n",
    "    \n",
    "    def set_info(self, dataset:pd.DataFrame):\n",
    "\n",
    "        (dataset.info())\n",
    "\n",
    "    def plot_feature(self, dataset:pd.DataFrame, featureName:str):\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "        dataset[['date_forecast', featureName]].set_index(\"date_forecast\").plot(ax=axs, title=featureName, color='red')\n",
    "       \n",
    "    def KNNImputing(self, datasets) :\n",
    "        from sklearn.impute import KNNImputer\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        imputed_sets = []\n",
    "\n",
    "        for set in tqdm(datasets): \n",
    "\n",
    "            # storing columns to lable after impute, also removing date column as this does not work with impute \n",
    "            cols = set.columns \n",
    "\n",
    "            if set.columns.__contains__(\"date_forecast\"): \n",
    "                dates = set[\"date_forecast\"]\n",
    "            \n",
    "            if set.columns.__contains__(\"date_calc\"): \n",
    "                set = set.drop(\"date_calc\", axis=1)\n",
    "\n",
    "            cols = set.columns.delete(0)\n",
    "\n",
    "            set_wo_date = set.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "            #imputing (estimating) missing values \n",
    "            imp = KNNImputer(n_neighbors=40, weights=\"distance\", )\n",
    "            imp.fit(set_wo_date)\n",
    "            set_wo_date = pd.DataFrame(imp.transform(set_wo_date), columns=imp.get_feature_names_out())\n",
    "            \n",
    "\n",
    "            # setting column lables basck\n",
    "            set = set_wo_date\n",
    "            \n",
    "            set[\"date_forecast\"] = dates\n",
    "\n",
    "            #sorting columns \n",
    "            cols = cols.tolist()\n",
    "            cols.insert(0, \"date_forecast\")\n",
    "\n",
    "            ## set = set.fillna(0.0)\n",
    "\n",
    "            imputed_sets.append(set)\n",
    "\n",
    "        return imputed_sets\n",
    "\n",
    "    def makima_interpolate(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        from scipy.interpolate import Akima1DInterpolator\n",
    "        # Extract non-missing values and their indices\n",
    "\n",
    "        dates = data[\"date_forecast\"]\n",
    "\n",
    "        non_nan_indices = data['pv_measurement'].dropna().index\n",
    "        non_nan_values = data['pv_measurement'].dropna().values\n",
    "\n",
    "        # Apply the Modified Akima Interpolation\n",
    "        akima = Akima1DInterpolator(non_nan_indices, non_nan_values)\n",
    "        interpolated_values = akima(data.index)\n",
    "\n",
    "        # Replace the original column with the interpolated values\n",
    "        data['pv_measurement'] = interpolated_values\n",
    "\n",
    "        data[data[\"pv_measurement\"]< 0] = 0 ; \n",
    "\n",
    "        data[\"date_forecast\"] = dates; \n",
    "\n",
    "        return data\n",
    "    \n",
    "    def normalize_data(self) : \n",
    "        from sklearn import preprocessing\n",
    "\n",
    "        if (self.data_A.empty) : \n",
    "            print( \"empty \")\n",
    "            relevant_sets = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\") and not attr.__contains__(\"data\") and not attr == 'amplitude' and not attr.__contains__(\"normalizing\")]\n",
    "\n",
    "        else: \n",
    "            print(\"not emptu\")\n",
    "            relevant_sets = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\") and attr.__contains__(\"data_\") and not attr.__contains__(\"_es\") and not attr.__contains__(\"obs\") or attr.__contains__(\"_estimated_\")]\n",
    "        self.__setattr__(\"normalizing_consts\", {})\n",
    "\n",
    "        print(relevant_sets)\n",
    "\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        normalizer = preprocessing.Normalizer()\n",
    "        \n",
    "\n",
    "        for att in relevant_sets: \n",
    "            set : pd.DataFrame = self.__getattribute__(att)\n",
    "\n",
    "            cols = set.columns \n",
    "\n",
    "            if set.columns.__contains__(\"date_forecast\"): \n",
    "                dates = set[\"date_forecast\"]\n",
    "            \n",
    "            if set.columns.__contains__(\"date_calc\"): \n",
    "                set = set.drop(\"date_calc\", axis=1)\n",
    "\n",
    "            cols = set.columns.delete(0)\n",
    "\n",
    "            set_wo_date = set.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "            x = set_wo_date.values\n",
    "\n",
    "            x_normalized = min_max_scaler.fit_transform(x)\n",
    "\n",
    "            self.normalizing_consts[att] = (set_wo_date.min(), np.abs(set_wo_date.max() - set_wo_date.min())) ## storing normalizing consts for later \n",
    "            \n",
    "            normalized_set = pd.DataFrame(x_normalized)\n",
    "\n",
    "            normalized_set.columns = cols\n",
    "\n",
    "            \n",
    "\n",
    "            self.__setattr__(att, normalized_set)\n",
    "        \n",
    "    def scaling(self, preds, location:str) : \n",
    "\n",
    "        \"\"\"\n",
    "        FORMAT OF PREDICTIONS SHOULD BE 1 COLUMN WITH PREDS\n",
    "\n",
    "        LOCATION: A B or C\n",
    "        \"\"\"\n",
    "\n",
    "        relevant_sets = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\") and not attr.__contains__(\"data\") and not attr == 'amplitude' and (attr.__contains__(\"train_a\") or attr.__contains__(\"train_b\") or attr.__contains__(\"train_c\"))]\n",
    "        if (self.data_A.empty) : \n",
    "            print( \"empty \")\n",
    "            relevant_sets = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\") and not attr.__contains__(\"data\") and not attr == 'amplitude' and not attr.__contains__(\"normalizing\")]\n",
    "\n",
    "        else: \n",
    "            print(\"not emptu\")\n",
    "            relevant_sets = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\") and attr.__contains__(\"data_\") and not attr.__contains__(\"es\") and not attr.__contains__(\"obs\")]\n",
    "\n",
    "        \n",
    "        loc_index = 0\n",
    "\n",
    "        if location.capitalize() == \"A\" :\n",
    "\n",
    "            loc_index = 0\n",
    "\n",
    "        elif location.capitalize() == \"B\": \n",
    "        \n",
    "            loc_index = 1\n",
    "\n",
    "        elif location.capitalize() == \"C\": \n",
    "\n",
    "            loc_index = 2\n",
    "           \n",
    "        elif ( location.capitalize() == \"D\"): \n",
    "\n",
    "            A = pd.DataFrame(preds[0:720])\n",
    "            B = pd.DataFrame(preds[720:2*720])\n",
    "            C = pd.DataFrame(preds[2*720:])\n",
    "\n",
    "            min = self.normalizing_consts[\"data_A\"][0][0]\n",
    "            diff = self.normalizing_consts[\"data_A\"][1][0]\n",
    "\n",
    "            scaled_A = (A+min) * diff\n",
    "\n",
    "\n",
    "\n",
    "            min = self.normalizing_consts[\"data_B\"][0][0]\n",
    "            diff = self.normalizing_consts[\"data_B\"][1][0]\n",
    "\n",
    "            scaled_B = (B+min) * diff\n",
    "\n",
    "\n",
    "\n",
    "            min = self.normalizing_consts[\"data_C\"][0][0]\n",
    "            diff = self.normalizing_consts[\"data_C\"][1][0]\n",
    "\n",
    "            scaled_C = (C+min) * diff\n",
    "\n",
    "\n",
    "            print(A.shape)\n",
    "            scaled_set = pd.concat([scaled_A, scaled_B, scaled_C])\n",
    "\n",
    "            return scaled_set\n",
    "\n",
    "        relevant_set = relevant_sets[loc_index]\n",
    "\n",
    "    \n",
    "        min = self.normalizing_consts[relevant_set][0][0]\n",
    "        diff = self.normalizing_consts[relevant_set][1][0]\n",
    "\n",
    "        scaled_set = (preds + min) * diff\n",
    "\n",
    "        return scaled_set\n",
    "    \n",
    "    def combine_overlap_BC(self): \n",
    "        import math\n",
    "        \"\"\"\n",
    "        This function is created for merging B and C to remove the nan values apparent when merging pv_measurement to the weather data\n",
    "        This is because of the observation that B and C overlap and where one is missing the other fills in. \n",
    "        Must run combine data first to create data_A B C\n",
    "        \"\"\"\n",
    "\n",
    "        original_B = self.data_B\n",
    "        original_C = self.data_C  \n",
    "\n",
    "        b2c_scaling = original_B[\"pv_measurement\"].max()/original_C[\"pv_measurement\"].max()\n",
    "\n",
    "        print(b2c_scaling)      \n",
    "\n",
    "        original_C[original_C.isnull()] = self.data_B\n",
    "        original_B[original_B.isnull()] = self.data_C\n",
    "\n",
    "        self.data_C = original_C.dropna()\n",
    "        self.data_B = original_B.dropna()\n",
    "\n",
    "    def sorting_columns_inMainSets(self):\n",
    "\n",
    "        A = self.data_A \n",
    "        cols = A.columns.tolist()\n",
    "\n",
    "        #sorting columns \n",
    "        cols.remove(\"date_forecast\")\n",
    "        cols.remove(\"pv_measurement\")\n",
    "        cols.insert(0, \"pv_measurement\")\n",
    "        cols.insert(0, \"date_forecast\")\n",
    "\n",
    "        A = A[cols]\n",
    "        self.data_A = A\n",
    "\n",
    "        #------------------------------------------------------------# \n",
    "\n",
    "        B = self.data_B\n",
    "        cols = B.columns.tolist()\n",
    "\n",
    "        #sorting columns \n",
    "        cols.remove(\"date_forecast\")\n",
    "        cols.remove(\"pv_measurement\")\n",
    "        cols.insert(0, \"pv_measurement\")\n",
    "        cols.insert(0, \"date_forecast\")\n",
    "\n",
    "        B = B[cols]\n",
    "        self.data_B = B \n",
    "\n",
    "        #------------------------------------------------------------#\n",
    "\n",
    "        C = self.data_C\n",
    "\n",
    "        cols = C.columns.tolist()\n",
    "\n",
    "        #sorting columns \n",
    "        cols.remove(\"date_forecast\")\n",
    "        cols.remove(\"pv_measurement\")\n",
    "        cols.insert(0, \"pv_measurement\")\n",
    "        cols.insert(0, \"date_forecast\")\n",
    "\n",
    "        C = C[cols]\n",
    "        self.data_C = C\n",
    "\n",
    "        #------------------------------------------------------------#\n",
    "\n",
    "        A = self.X_test_estimated_a\n",
    "\n",
    "        cols = A.columns.tolist()\n",
    "\n",
    "        #sorting columns \n",
    "        cols.remove(\"date_forecast\")\n",
    "        cols.insert(0, \"date_forecast\")\n",
    "\n",
    "        A = A[cols]\n",
    "        self.X_test_estimated_a = A\n",
    "\n",
    "        #------------------------------------------------------------#\n",
    "\n",
    "        B = self.X_test_estimated_b\n",
    "\n",
    "        cols = B.columns.tolist()\n",
    "\n",
    "        #sorting columns \n",
    "        cols.remove(\"date_forecast\")\n",
    "        cols.insert(0, \"date_forecast\")\n",
    "\n",
    "        B = B[cols]\n",
    "        self.X_test_estimated_b = B\n",
    "\n",
    "        #------------------------------------------------------------#\n",
    "\n",
    "        C = self.X_test_estimated_c\n",
    "\n",
    "        cols = C.columns.tolist()\n",
    "\n",
    "        #sorting columns \n",
    "        cols.remove(\"date_forecast\")\n",
    "        cols.insert(0, \"date_forecast\")\n",
    "\n",
    "        C = C[cols]\n",
    "        self.X_test_estimated_c = C\n",
    "\n",
    "        #------------------------------------------------------------#\n",
    "\n",
    "        if ( not self.data.empty) : \n",
    "            data = self.data\n",
    "\n",
    "            cols = data.columns.tolist()\n",
    "\n",
    "            #sorting columns \n",
    "            cols.remove(\"date_forecast\")\n",
    "            cols.insert(0, \"date_forecast\")\n",
    "\n",
    "            data = data[cols]\n",
    "            self.data = data\n",
    "\n",
    "    def remove_constant_periods(self, period_length, ignore_values=[]) :\n",
    "        from helpers import find_const_interval\n",
    "\n",
    "        train_a = self.data_A.reset_index(drop=True)\n",
    "        train_b = self.data_B.reset_index(drop=True)\n",
    "        train_c = self.data_C.reset_index(drop=True)\n",
    "\n",
    "        y_train_a_const_idx, ca = find_const_interval(train_a, 'pv_measurement', period_length, ignore_values)\n",
    "        print('y_train_a anomalies:',ca)\n",
    "\n",
    "        y_train_b_const_idx, cb = find_const_interval(train_b, 'pv_measurement', period_length, ignore_values)\n",
    "        print('y_train_b anomalies:',cb)\n",
    "\n",
    "        y_train_c_const_idx, cc = find_const_interval(train_c, 'pv_measurement', period_length, ignore_values)\n",
    "        print('y_train_c anomalies:',cc)\n",
    "\n",
    "\n",
    "        date_forecast_a_const = train_a.iloc[y_train_a_const_idx]['date_forecast']\n",
    "        date_forecast_a_const_values = date_forecast_a_const.values\n",
    "        train_a = train_a[~train_a['date_forecast'].isin(date_forecast_a_const_values)]\n",
    "\n",
    "        date_forecast_b_const = train_b.iloc[y_train_b_const_idx]['date_forecast']\n",
    "        date_forecast_b_const_values = date_forecast_b_const.values\n",
    "        train_b = train_b[~train_b['date_forecast'].isin(date_forecast_b_const_values)]\n",
    "\n",
    "        date_forecast_c_const = train_c.iloc[y_train_c_const_idx]['date_forecast']\n",
    "        date_forecast_c_const_values = date_forecast_c_const.values\n",
    "        train_c = train_c[~train_c['date_forecast'].isin(date_forecast_c_const_values)]\n",
    "\n",
    "        self.data_A = train_a\n",
    "        self.data_B = train_b\n",
    "        self.data_C = train_c\n",
    "                \n",
    "    def split_timeseries(self, X, y, num_splits):\n",
    "\n",
    "        num_rows = X.shape[0]\n",
    "\n",
    "        split_length = num_rows // num_splits\n",
    "\n",
    "        all_splits = []\n",
    "\n",
    "        end_idx = split_length\n",
    "\n",
    "        all_splits.append([np.arange(0, round(split_length*0.9), 1), np.arange(round(split_length*0.9), split_length, 1)])\n",
    "\n",
    "\n",
    "        while 2*end_idx < num_rows: \n",
    "\n",
    "            all_splits.append([np.arange(round(end_idx), end_idx + round(0.9*end_idx), 1), np.arange(round(end_idx*0.9)+end_idx, 2*end_idx, 1)])\n",
    "\n",
    "            end_idx += split_length\n",
    "\n",
    "        print(end_idx + split_length)\n",
    "        rest = num_rows - end_idx \n",
    "\n",
    "        all_splits.append([np.arange(round(2), )])\n",
    "\n",
    "        print(all_splits) \n",
    "\n",
    "    def add_location(self, dataset:pd.DataFrame, location):\n",
    "\n",
    "        if location == \"A\": loc = 0 \n",
    "        if location == \"B\": loc = 1 \n",
    "        if location == \"C\": loc = 2\n",
    "\n",
    "        index = dataset.index\n",
    "\n",
    "        loc_column = pd.DataFrame()\n",
    "        loc_column.index = index\n",
    "        loc_column[\"location\"] = loc\n",
    "\n",
    "        dataset[\"location\"] = loc\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def add_lag_feature(self, target_attribute, lag):\n",
    "\n",
    "        lag_attribute = target_attribute + \"_lag_\" + str(lag)\n",
    "\n",
    "        self.data_A[lag_attribute] = self.data_A[target_attribute].shift(lag).fillna(0)\n",
    "        self.data_B[lag_attribute] = self.data_B[target_attribute].shift(lag).fillna(0)\n",
    "        self.data_C[lag_attribute] = self.data_C[target_attribute].shift(lag).fillna(0)\n",
    "\n",
    "        self.X_test_estimated_a[lag_attribute] = self.X_test_estimated_a[target_attribute].shift(lag).fillna(0)\n",
    "        self.X_test_estimated_b[lag_attribute] = self.X_test_estimated_b[target_attribute].shift(lag).fillna(0)\n",
    "        self.X_test_estimated_c[lag_attribute] = self.X_test_estimated_c[target_attribute].shift(lag).fillna(0)\n",
    "\n",
    "        \n",
    "\n",
    "    def combine_all_data(self): \n",
    "\n",
    "        relevant_sets_A = [attr for attr in dir(self) if attr.__eq__(\"data_A\") or attr.__eq__(\"X_test_estimated_a\")]\n",
    "        relevant_sets_B = [attr for attr in dir(self) if attr.__eq__(\"data_B\") or attr.__eq__(\"X_test_estimated_b\")]\n",
    "        relevant_sets_C = [attr for attr in dir(self) if attr.__eq__(\"data_C\") or attr.__eq__(\"X_test_estimated_c\")]\n",
    "\n",
    "        print(relevant_sets_A)\n",
    "        print(relevant_sets_B)\n",
    "        print(relevant_sets_C)\n",
    "\n",
    "\n",
    "        for set in relevant_sets_A : \n",
    "            self.__setattr__(set, self.add_location(self.__getattribute__(set), \"A\"))\n",
    "        for set in relevant_sets_B : \n",
    "            self.__setattr__(set, self.add_location(self.__getattribute__(set), \"B\"))\n",
    "        for set in relevant_sets_C : \n",
    "            self.__setattr__(set, self.add_location(self.__getattribute__(set), \"C\"))\n",
    "\n",
    "\n",
    "        self.data = pd.concat([self.data_A, self.data_B, self.data_C], ignore_index=True)\n",
    "        self.X_test_estimated = pd.concat([self.X_test_estimated_a, self.X_test_estimated_b, self.X_test_estimated_c], ignore_index=True)\n",
    "\n",
    "    def remove_outliers_z_score(self, data:pd.DataFrame, threshold=3) : \n",
    "        \n",
    "        from scipy import stats\n",
    "\n",
    "        before = self.data[\"diffuse_rad:W\"][0:3*720]\n",
    "\n",
    "        dates = None \n",
    "\n",
    "        if \"date_forecast\" in data.columns: \n",
    "\n",
    "            dates = data[\"date_forecast\"]\n",
    "            data = data.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "        z_scores = stats.zscore(data.astype(float))\n",
    "\n",
    "        outliers = data[z_scores > threshold]\n",
    "        #outliers = outliers.drop(\"index\", axis=1)\n",
    "        outliers = outliers.notna()\n",
    "\n",
    "        indexx = None\n",
    "\n",
    "        if (len(outliers) > 1): \n",
    "\n",
    "            for outlier in outliers: \n",
    "                # print(outliers[outlier])\n",
    "\n",
    "                # print(data[outlier])\n",
    "\n",
    "                if (outlier != \"index\"):\n",
    "\n",
    "                    index = data[outlier].loc[outliers[outlier] == True]\n",
    "\n",
    "                    data[outlier] = data[outlier].replace(index.index, np.nan)\n",
    "\n",
    "\n",
    "                    indexx = index\n",
    "            \n",
    "\n",
    "            data[\"date_forecast\"] = dates\n",
    "\n",
    "            data = self.KNNImputing([data])[0]\n",
    "            data = data.reset_index(drop=True)\n",
    "            self.sorting_columns_inMainSets()\n",
    "            print(self.set_info(self.data))\n",
    "            \n",
    "            x = np.arange(0, 3*720, 1)\n",
    "            fig, axs = plt.subplots(1, 1, figsize=(20, 10))\n",
    "            plt.plot(x, before, c=\"blue\", label=\"before\")            \n",
    "            plt.plot(x, data[\"diffuse_rad:W\"][0:3*720], c=\"orange\", label=\"after\") \n",
    "            plt.legend()           \n",
    "            plt.show()\n",
    "            return data\n",
    "\n",
    "        else: \n",
    "            print(\"no outliers\")\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init Datamanager\n",
    "\n",
    "from data_prep import data_manager as DM\n",
    "\n",
    "# instanciate a new datamanager \n",
    "dm = DM.Data_Manager()\n",
    "# loads all data into the datamanager \n",
    "dm.data_loader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove nan columns\n",
    "\n",
    "dm.X_train_observed_a = dm.X_train_observed_a.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_train_observed_b = dm.X_train_observed_b.drop(\"snow_density:kgm3\", axis=1) \n",
    "dm.X_train_observed_c = dm.X_train_observed_c.drop(\"snow_density:kgm3\", axis=1) \n",
    "\n",
    "dm.X_train_estimated_a = dm.X_train_estimated_a.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_train_estimated_b = dm.X_train_estimated_b.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_train_estimated_c = dm.X_train_estimated_c.drop(\"snow_density:kgm3\", axis=1)\n",
    "\n",
    "dm.X_train_observed_a = dm.X_train_observed_a.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_train_observed_b = dm.X_train_observed_b.drop(\"ceiling_height_agl:m\", axis=1) \n",
    "dm.X_train_observed_c = dm.X_train_observed_c.drop(\"ceiling_height_agl:m\", axis=1) \n",
    "dm.X_train_estimated_a = dm.X_train_estimated_a.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_train_estimated_b = dm.X_train_estimated_b.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_train_estimated_c = dm.X_train_estimated_c.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "\n",
    "dm.X_train_observed_a = dm.X_train_observed_a.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_train_observed_b = dm.X_train_observed_b.drop(\"cloud_base_agl:m\", axis=1) \n",
    "dm.X_train_observed_c = dm.X_train_observed_c.drop(\"cloud_base_agl:m\", axis=1) \n",
    "dm.X_train_estimated_a = dm.X_train_estimated_a.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_train_estimated_b = dm.X_train_estimated_b.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_train_estimated_c = dm.X_train_estimated_c.drop(\"cloud_base_agl:m\", axis=1)\n",
    "\n",
    "dm.X_test_estimated_a = dm.X_test_estimated_a.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_test_estimated_b = dm.X_test_estimated_b.drop(\"cloud_base_agl:m\", axis=1) \n",
    "dm.X_test_estimated_c = dm.X_test_estimated_c.drop(\"cloud_base_agl:m\", axis=1) \n",
    "\n",
    "dm.X_test_estimated_a = dm.X_test_estimated_a.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_test_estimated_b = dm.X_test_estimated_b.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_test_estimated_c = dm.X_test_estimated_c.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "\n",
    "dm.X_test_estimated_a = dm.X_test_estimated_a.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_test_estimated_b = dm.X_test_estimated_b.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_test_estimated_c = dm.X_test_estimated_c.drop(\"snow_density:kgm3\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine into data_A/B/C\n",
    "\n",
    "dm.data_A, dm.data_B, dm.data_C = dm.combine_data()\n",
    "dm.sorting_columns_inMainSets()\n",
    "\n",
    "print(dm.data_A.shape, dm.data_B.shape, dm.data_C.shape, dm.data.shape)\n",
    "\n",
    "dates_A = dm.data_A[\"date_forecast\"]\n",
    "dates_B = dm.data_B[\"date_forecast\"]\n",
    "dates_C = dm.data_C[\"date_forecast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lag features\n",
    "\n",
    "hours = 0\n",
    "days = 1\n",
    "placeholder = ['direct_rad:W', 'sun_azimuth:d', 'clear_sky_energy_1h:J']\n",
    "features = [placeholder[0]]\n",
    "print(features)\n",
    "lag = np.append(-1*np.arange(1, hours+1), -1*np.arange(24, 24*days+1, 24))\n",
    "for l in lag:\n",
    "    for feature in features:\n",
    "        dm.add_lag_feature(feature, l)\n",
    "\n",
    "dm.add_lag_feature('direct_rad:W', -23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing const y values\n",
    "\n",
    "dm.remove_constant_periods(12)\n",
    "# dm.remove_constant_periods(30) # HR FREQUENCY\n",
    "# dm.remove_constant_periods(200) # 15 MIN FREQUENCY\n",
    "\n",
    "dm.remove_constant_periods(3, [0]) # Keep this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BC Donation\n",
    "from helpers import donate_missing_rows\n",
    "\n",
    "updated_b, count_b = donate_missing_rows(dm.data_B, dm.data_C)\n",
    "print('donated rows from C to B: ', count_b)\n",
    "updated_c, count_c = donate_missing_rows(dm.data_C, dm.data_B)\n",
    "print('donated rows from B to C: ', count_c)\n",
    "\n",
    "dm.data_B = updated_b\n",
    "dm.data_C = updated_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Months\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def month_func(x): \n",
    "    return -6* np.cos(2*np.pi/12 * x) + 6\n",
    "\n",
    "\n",
    "dm.data_A[\"month\"] = month_func((dm.data_A[\"date_forecast\"]).dt.month)\n",
    "dm.data_B[\"month\"] = month_func((dm.data_B[\"date_forecast\"]).dt.month)\n",
    "dm.data_C[\"month\"] = month_func((dm.data_C[\"date_forecast\"]).dt.month)\n",
    "#dm.data[\"month\"] = month_func((dm.data[\"date_forecast\"]).dt.month)\n",
    "\n",
    "dm.X_test_estimated_a[\"month\"] = month_func((dm.X_test_estimated_a[\"date_forecast\"]).dt.month)\n",
    "dm.X_test_estimated_b[\"month\"] = month_func((dm.X_test_estimated_b[\"date_forecast\"]).dt.month)\n",
    "dm.X_test_estimated_c[\"month\"] = month_func((dm.X_test_estimated_c[\"date_forecast\"]).dt.month)\n",
    "#dm.X_test_estimated[\"month\"] = month_func((dm.X_test_estimated[\"date_forecast\"]).dt.month)\n",
    "\n",
    "\n",
    "\n",
    "dm.plot_feature(dm.data_A, \"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time of day\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def hour_func(x): \n",
    "    return -12*np.cos(2*np.pi/24 * x) + 12\n",
    "\n",
    "dm.X_test_estimated_a[\"hours\"] = hour_func(dm.X_test_estimated_a[\"date_forecast\"].dt.hour)\n",
    "dm.X_test_estimated_b[\"hours\"] = hour_func(dm.X_test_estimated_b[\"date_forecast\"].dt.hour)\n",
    "dm.X_test_estimated_c[\"hours\"] = hour_func(dm.X_test_estimated_c[\"date_forecast\"].dt.hour)\n",
    "#dm.X_test_estimated[\"hours\"] = hour_func(dm.X_test_estimated[\"date_forecast\"].dt.hour)\n",
    "\n",
    "dm.data_A[\"hours\"] =  hour_func(dm.data_A[\"date_forecast\"].dt.hour)\n",
    "dm.data_B[\"hours\"] = hour_func(dm.data_B[\"date_forecast\"].dt.hour)\n",
    "dm.data_C[\"hours\"] = hour_func(dm.data_C[\"date_forecast\"].dt.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Week number \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def week_func(x): \n",
    "    return -26*np.cos(2*np.pi/52 * x) + 26\n",
    "\n",
    "dm.X_test_estimated_a[\"week_number\"] = week_func(dm.X_test_estimated_a[\"date_forecast\"].dt.isocalendar().week)\n",
    "dm.X_test_estimated_b[\"week_number\"] = week_func(dm.X_test_estimated_b[\"date_forecast\"].dt.isocalendar().week)\n",
    "dm.X_test_estimated_c[\"week_number\"] = week_func(dm.X_test_estimated_c[\"date_forecast\"].dt.isocalendar().week)\n",
    "#dm.X_test_estimated[\"week_number\"] = wweek_funceek_func(dm.X_test_estimated[\"date_forecast\"].dt.isocalendar().week)\n",
    "\n",
    "dm.data_A[\"week_number\"] =  week_func(dm.data_A[\"date_forecast\"].dt.isocalendar().week)\n",
    "dm.data_B[\"week_number\"] = week_func(dm.data_B[\"date_forecast\"].dt.isocalendar().week)\n",
    "dm.data_C[\"week_number\"] = week_func(dm.data_C[\"date_forecast\"].dt.isocalendar().week)\n",
    "##dm.data[\"week_number\"] = week_func(dm.data[\"date_forecast\"].dt.isocalendar().week)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Day of year \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def day_func(x): \n",
    "    return -182.5*np.cos(2*np.pi/365 * x) + 182.5\n",
    "\n",
    "dm.X_test_estimated_a[\"day_year\"] = day_func(dm.X_test_estimated_a[\"date_forecast\"].dt.dayofyear)\n",
    "dm.X_test_estimated_b[\"day_year\"] = day_func(dm.X_test_estimated_b[\"date_forecast\"].dt.dayofyear)\n",
    "dm.X_test_estimated_c[\"day_year\"] = day_func(dm.X_test_estimated_c[\"date_forecast\"].dt.dayofyear)\n",
    "#dm.X_test_estimated[\"day_year\"] = day_func(dm.X_test_estimated[\"date_forecast\"].dt.dayofyear)\n",
    "\n",
    "dm.data_A[\"day_year\"] =  day_func(dm.data_A[\"date_forecast\"].dt.dayofyear)\n",
    "dm.data_B[\"day_year\"] = day_func(dm.data_B[\"date_forecast\"].dt.dayofyear)\n",
    "dm.data_C[\"day_year\"] = day_func(dm.data_C[\"date_forecast\"].dt.dayofyear)\n",
    "#dm.data[\"day_year\"] = day_func(dm.data[\"date_forecast\"].dt.dayofyear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum of radiation \n",
    "\n",
    "dm.data_A[\"sum_rad:W\"] = (dm.data_A[\"clear_sky_rad:W\"] + dm.data_A[\"diffuse_rad:W\"] + dm.data_A[\"direct_rad:W\"])/3\n",
    "dm.data_B[\"sum_rad:W\"] = (dm.data_B[\"clear_sky_rad:W\"] + dm.data_B[\"diffuse_rad:W\"] + dm.data_B[\"direct_rad:W\"])/3\n",
    "dm.data_C[\"sum_rad:W\"] = (dm.data_C[\"clear_sky_rad:W\"] + dm.data_C[\"diffuse_rad:W\"] + dm.data_C[\"direct_rad:W\"])/3\n",
    "# #dm.data[\"sum_rad:W\"] = (dm.data[\"clear_sky_rad:W\"] + dm.data[\"diffuse_rad:W\"] + dm.data[\"direct_rad:W\"])/3\n",
    "\n",
    "dm.X_test_estimated_a[\"sum_rad:W\"] = (dm.X_test_estimated_a[\"clear_sky_rad:W\"] + dm.X_test_estimated_a[\"diffuse_rad:W\"] + dm.X_test_estimated_a[\"direct_rad:W\"])/3\n",
    "dm.X_test_estimated_b[\"sum_rad:W\"] = (dm.X_test_estimated_b[\"clear_sky_rad:W\"] + dm.X_test_estimated_b[\"diffuse_rad:W\"] + dm.X_test_estimated_b[\"direct_rad:W\"])/3\n",
    "dm.X_test_estimated_c[\"sum_rad:W\"] = (dm.X_test_estimated_c[\"clear_sky_rad:W\"] + dm.X_test_estimated_c[\"diffuse_rad:W\"] + dm.X_test_estimated_c[\"direct_rad:W\"])/3\n",
    "# #dm.X_test_estimated[\"sum_rad:W\"] = (dm.X_test_estimated[\"clear_sky_rad:W\"] + dm.X_test_estimated[\"diffuse_rad:W\"] + dm.X_test_estimated[\"direct_rad:W\"])/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
