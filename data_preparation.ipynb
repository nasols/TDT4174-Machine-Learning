{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that handles data loading! \n",
    "\n",
    "This notebook is used to initially load and prepare data. This means loading into a datamanager, merging data (if wanted), and doing any preliminary alterations to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep import data_manager as DM\n",
    "\n",
    "# instanciate a new datamanager \n",
    "dm = DM.Data_Manager()\n",
    "# loads all data into the datamanager \n",
    "dm.data_loader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing features with null values\n",
    "\n",
    "This mainly removes three features, these either have alot of null values or are not important for our final model, making it ok to remove either way.\n",
    "This also removes the need for imputing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DATA NULLS\n",
      " \n",
      "x_train_observed\n",
      "ceiling_height_agl:m     22247\n",
      "cloud_base_agl:m          8066\n",
      "snow_density:kgm3       115945\n",
      "dtype: int64\n",
      " \n",
      "x_train_estimated\n",
      "ceiling_height_agl:m     3919\n",
      "cloud_base_agl:m         2094\n",
      "snow_density:kgm3       15769\n",
      "dtype: int64\n",
      " \n",
      "x_test_estimated\n",
      "ceiling_height_agl:m     793\n",
      "cloud_base_agl:m         298\n",
      "snow_density:kgm3       2880\n",
      "dtype: int64\n",
      " \n",
      "data_A\n",
      "Series([], dtype: float64)\n",
      " \n",
      "B DATA NULLS\n",
      " \n",
      "x_train_observed\n",
      "ceiling_height_agl:m     18772\n",
      "cloud_base_agl:m          7473\n",
      "snow_density:kgm3       111201\n",
      "dtype: int64\n",
      " \n",
      "x_train_estimated\n",
      "ceiling_height_agl:m     3689\n",
      "cloud_base_agl:m         1963\n",
      "snow_density:kgm3       15713\n",
      "dtype: int64\n",
      " \n",
      "x_test_estimated\n",
      "ceiling_height_agl:m     755\n",
      "cloud_base_agl:m         277\n",
      "snow_density:kgm3       2880\n",
      "dtype: int64\n",
      " \n",
      "data_B\n",
      "Series([], dtype: float64)\n",
      " \n",
      "C DATA NULLS\n",
      " \n",
      "x_train_observed\n",
      "ceiling_height_agl:m     19923\n",
      "cloud_base_agl:m          8512\n",
      "snow_density:kgm3       107593\n",
      "dtype: int64\n",
      " \n",
      "x_train_estimated\n",
      "ceiling_height_agl:m     4596\n",
      "cloud_base_agl:m         2804\n",
      "snow_density:kgm3       13898\n",
      "dtype: int64\n",
      " \n",
      "x_test_estimated\n",
      "ceiling_height_agl:m     849\n",
      "cloud_base_agl:m         375\n",
      "snow_density:kgm3       2880\n",
      "dtype: int64\n",
      " \n",
      "data_C\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"A DATA NULLS\")\n",
    "print (\" \")\n",
    "print('x_train_observed')\n",
    "print(dm.X_train_observed_a.isnull().sum()[dm.X_train_observed_a.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('x_train_estimated')\n",
    "print(dm.X_train_estimated_a.isnull().sum()[dm.X_train_estimated_a.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('x_test_estimated')\n",
    "print(dm.X_test_estimated_a.isnull().sum()[dm.X_test_estimated_a.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('data_A')\n",
    "print(dm.data_A.isnull().sum()[dm.data_A.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print(\"B DATA NULLS\")\n",
    "print (\" \")\n",
    "print('x_train_observed')\n",
    "print(dm.X_train_observed_b.isnull().sum()[dm.X_train_observed_b.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('x_train_estimated')\n",
    "print(dm.X_train_estimated_b.isnull().sum()[dm.X_train_estimated_b.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('x_test_estimated')\n",
    "print(dm.X_test_estimated_b.isnull().sum()[dm.X_test_estimated_b.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('data_B')\n",
    "print(dm.data_B.isnull().sum()[dm.data_B.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print(\"C DATA NULLS\")\n",
    "print (\" \")\n",
    "print('x_train_observed')\n",
    "print(dm.X_train_observed_c.isnull().sum()[dm.X_train_observed_c.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('x_train_estimated')\n",
    "print(dm.X_train_estimated_c.isnull().sum()[dm.X_train_estimated_c.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('x_test_estimated')\n",
    "print(dm.X_test_estimated_c.isnull().sum()[dm.X_test_estimated_c.isnull().sum() > 0])\n",
    "print (\" \")\n",
    "print('data_C')\n",
    "print(dm.data_C.isnull().sum()[dm.data_C.isnull().sum() > 0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.X_train_observed_a = dm.X_train_observed_a.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_train_observed_b = dm.X_train_observed_b.drop(\"snow_density:kgm3\", axis=1) \n",
    "dm.X_train_observed_c = dm.X_train_observed_c.drop(\"snow_density:kgm3\", axis=1) \n",
    "\n",
    "dm.X_train_estimated_a = dm.X_train_estimated_a.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_train_estimated_b = dm.X_train_estimated_b.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_train_estimated_c = dm.X_train_estimated_c.drop(\"snow_density:kgm3\", axis=1)\n",
    "\n",
    "dm.X_train_observed_a = dm.X_train_observed_a.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_train_observed_b = dm.X_train_observed_b.drop(\"ceiling_height_agl:m\", axis=1) \n",
    "dm.X_train_observed_c = dm.X_train_observed_c.drop(\"ceiling_height_agl:m\", axis=1) \n",
    "dm.X_train_estimated_a = dm.X_train_estimated_a.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_train_estimated_b = dm.X_train_estimated_b.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_train_estimated_c = dm.X_train_estimated_c.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "\n",
    "dm.X_train_observed_a = dm.X_train_observed_a.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_train_observed_b = dm.X_train_observed_b.drop(\"cloud_base_agl:m\", axis=1) \n",
    "dm.X_train_observed_c = dm.X_train_observed_c.drop(\"cloud_base_agl:m\", axis=1) \n",
    "dm.X_train_estimated_a = dm.X_train_estimated_a.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_train_estimated_b = dm.X_train_estimated_b.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_train_estimated_c = dm.X_train_estimated_c.drop(\"cloud_base_agl:m\", axis=1)\n",
    "\n",
    "dm.X_test_estimated_a = dm.X_test_estimated_a.drop(\"cloud_base_agl:m\", axis=1)\n",
    "dm.X_test_estimated_b = dm.X_test_estimated_b.drop(\"cloud_base_agl:m\", axis=1) \n",
    "dm.X_test_estimated_c = dm.X_test_estimated_c.drop(\"cloud_base_agl:m\", axis=1) \n",
    "\n",
    "dm.X_test_estimated_a = dm.X_test_estimated_a.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_test_estimated_b = dm.X_test_estimated_b.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "dm.X_test_estimated_c = dm.X_test_estimated_c.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "\n",
    "dm.X_test_estimated_a = dm.X_test_estimated_a.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_test_estimated_b = dm.X_test_estimated_b.drop(\"snow_density:kgm3\", axis=1)\n",
    "dm.X_test_estimated_c = dm.X_test_estimated_c.drop(\"snow_density:kgm3\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we look at sample rates\n",
    "\n",
    "the training weather data is sampled every 15 minutes, whilst the pv_measurements are every hour. \n",
    "We can either \n",
    "1. sample down. Making weather data be every hour using the mean value of every hour \n",
    "2. sample up. Making the pv_measurement every 15 min instead of every hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_forecast</th>\n",
       "      <th>pv_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-02 22:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-02 23:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-03 00:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-06-03 01:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-03 02:00:00</td>\n",
       "      <td>19.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34080</th>\n",
       "      <td>2023-04-30 19:00:00</td>\n",
       "      <td>9.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34081</th>\n",
       "      <td>2023-04-30 20:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34082</th>\n",
       "      <td>2023-04-30 21:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34083</th>\n",
       "      <td>2023-04-30 22:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34084</th>\n",
       "      <td>2023-04-30 23:00:00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34085 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_forecast  pv_measurement\n",
       "0     2019-06-02 22:00:00            0.00\n",
       "1     2019-06-02 23:00:00            0.00\n",
       "2     2019-06-03 00:00:00            0.00\n",
       "3     2019-06-03 01:00:00            0.00\n",
       "4     2019-06-03 02:00:00           19.36\n",
       "...                   ...             ...\n",
       "34080 2023-04-30 19:00:00            9.02\n",
       "34081 2023-04-30 20:00:00            0.00\n",
       "34082 2023-04-30 21:00:00            0.00\n",
       "34083 2023-04-30 22:00:00            0.00\n",
       "34084 2023-04-30 23:00:00            0.00\n",
       "\n",
       "[34085 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample down\n",
    "\n",
    "resamples = dm.resample_data([dm.X_train_observed_a, \n",
    "                                   dm.X_train_observed_b, \n",
    "                                   dm.X_train_observed_c, \n",
    "                                   dm.X_train_estimated_a,\n",
    "                                   dm.X_train_estimated_b,\n",
    "                                   dm.X_train_estimated_c,\n",
    "                                   dm.X_test_estimated_a,\n",
    "                                   dm.X_test_estimated_b,\n",
    "                                   dm.X_test_estimated_c], \"H\")\n",
    "\n",
    "dm.X_train_observed_a = resamples[0]\n",
    "dm.X_train_observed_b = resamples[1] \n",
    "dm.X_train_observed_c = resamples[2] \n",
    "dm.X_train_estimated_a = resamples[3]\n",
    "dm.X_train_estimated_b = resamples[4]\n",
    "dm.X_train_estimated_c = resamples[5]\n",
    "dm.X_test_estimated_a = resamples[6]\n",
    "dm.X_test_estimated_b = resamples[7]\n",
    "dm.X_test_estimated_c = resamples[8]\n",
    "\n",
    "\n",
    "\n",
    "# Sample up\n",
    "\n",
    "# resamples = dm.resample_data([\n",
    "#                             dm.X_test_estimated_a,\n",
    "#                             dm.X_test_estimated_b,\n",
    "#                             dm.X_test_estimated_c], \"H\")\n",
    "\n",
    "\n",
    "# dm.X_test_estimated_a = resamples[0]\n",
    "# dm.X_test_estimated_b = resamples[1]\n",
    "# dm.X_test_estimated_c = resamples[2]\n",
    "\n",
    "# resamples = dm.resample_data([dm.train_a, dm.train_b, dm.train_c], \"15T\")\n",
    "\n",
    "# train_a_15min = resamples[0]\n",
    "# train_b_15min = resamples[1]\n",
    "# train_c_15min = resamples[2]\n",
    "\n",
    "# dm.train_a = resamples[0]\n",
    "# dm.train_b = resamples[1]\n",
    "# dm.train_c = resamples[2]\n",
    "\n",
    "\n",
    "dm.train_a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data into one training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34061, 44) (32819, 44) (26071, 44) (0, 0)\n"
     ]
    }
   ],
   "source": [
    "dm.data_A, dm.data_B, dm.data_C = dm.combine_data()\n",
    "dm.sorting_columns_inMainSets()\n",
    "\n",
    "print(dm.data_A.shape, dm.data_B.shape, dm.data_C.shape, dm.data.shape)\n",
    "\n",
    "dates_A = dm.data_A[\"date_forecast\"]\n",
    "dates_B = dm.data_B[\"date_forecast\"]\n",
    "dates_C = dm.data_C[\"date_forecast\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loghe\\OneDrive\\Dokumenter\\Skole\\Host23\\ML\\main_project\\TDT4174-Machine-Learning\\data_prep\\data_manager.py:688: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X_test_estimated_a[lag_attribute] = self.X_test_estimated_a[target_attribute].shift(lag).fillna(0)\n",
      "c:\\Users\\loghe\\OneDrive\\Dokumenter\\Skole\\Host23\\ML\\main_project\\TDT4174-Machine-Learning\\data_prep\\data_manager.py:689: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X_test_estimated_b[lag_attribute] = self.X_test_estimated_b[target_attribute].shift(lag).fillna(0)\n",
      "c:\\Users\\loghe\\OneDrive\\Dokumenter\\Skole\\Host23\\ML\\main_project\\TDT4174-Machine-Learning\\data_prep\\data_manager.py:690: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X_test_estimated_c[lag_attribute] = self.X_test_estimated_c[target_attribute].shift(lag).fillna(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hours = 0\n",
    "# days = 1\n",
    "# placeholder = ['direct_rad:W', 'sun_azimuth:d', 'clear_sky_energy_1h:J']\n",
    "# features = [placeholder[0]]\n",
    "# print(features)\n",
    "# lag = np.append(-1*np.arange(1, hours+1), -1*np.arange(24, 24*days+1, 24))\n",
    "# for l in lag:\n",
    "#     for feature in features:\n",
    "#         dm.add_lag_feature(feature, l)\n",
    "\n",
    "dm.add_lag_feature('direct_rad:W', -23)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing const y values\n",
    "This is most likely due to errors in measurement device. Since it doesn't make sense to train on data with errors, we will remove the entire rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_a anomalies: 673\n",
      "y_train_b anomalies: 651\n",
      "y_train_c anomalies: 511\n",
      "y_train_a anomalies: 0\n",
      "y_train_b anomalies: 15\n",
      "y_train_c anomalies: 58\n"
     ]
    }
   ],
   "source": [
    "dm.remove_constant_periods(12)\n",
    "# dm.remove_constant_periods(30) # HR FREQUENCY\n",
    "# dm.remove_constant_periods(200) # 15 MIN FREQUENCY\n",
    "\n",
    "dm.remove_constant_periods(3, [0]) # Keep this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC Donation\n",
    "At this point dataset B and C has a lot of missing values. We will exploit the fact that the two training sets are quite similar and donate missing values to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donated rows from C to B:  3213\n",
      "donated rows from B to C:  6022\n"
     ]
    }
   ],
   "source": [
    "from helpers import donate_missing_rows\n",
    "\n",
    "updated_b, count_b = donate_missing_rows(dm.data_B, dm.data_C)\n",
    "print('donated rows from C to B: ', count_b)\n",
    "updated_c, count_c = donate_missing_rows(dm.data_C, dm.data_B)\n",
    "print('donated rows from B to C: ', count_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set donated data\n",
    "dm.data_B = updated_b\n",
    "dm.data_C = updated_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dm' (Data_Manager)\n"
     ]
    }
   ],
   "source": [
    "%store dm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
