{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot function\n",
    "\n",
    "def plot_feature(dataset:pd.DataFrame, featureName:str):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "    dataset[['date_forecast', featureName]].set_index(\"date_forecast\").plot(ax=axs, title=featureName, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "data_A = pd.read_csv(\"current_csv_files/data_A_fix.csv\", index_col='Unnamed: 0')\n",
    "data_B = pd.read_csv(\"current_csv_files/data_B_fix.csv\", index_col='Unnamed: 0')\n",
    "data_C = pd.read_csv(\"current_csv_files/data_C_fix.csv\", index_col='Unnamed: 0')\n",
    "\n",
    "test_A = pd.read_csv(\"current_csv_files/X_test_A_fix.csv\", index_col='Unnamed: 0')\n",
    "test_B = pd.read_csv(\"current_csv_files/X_test_B_fix.csv\", index_col='Unnamed: 0')\n",
    "test_C = pd.read_csv(\"current_csv_files/X_test_C_fix.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark estimated data\n",
    "\n",
    "est_dates_A = pd.to_datetime(pd.read_parquet('../A/X_train_estimated.parquet')['date_forecast'])\n",
    "est_dates_B = pd.to_datetime(pd.read_parquet('../B/X_train_estimated.parquet')['date_forecast'])\n",
    "est_dates_C = pd.to_datetime(pd.read_parquet('../C/X_train_estimated.parquet')['date_forecast'])\n",
    "\n",
    "data_A['est'] = (pd.to_datetime(data_A['date_forecast']).isin(est_dates_A)).astype(int)\n",
    "data_B['est'] = (pd.to_datetime(data_B['date_forecast']).isin(est_dates_B)).astype(int)\n",
    "data_C['est'] = (pd.to_datetime(data_C['date_forecast']).isin(est_dates_C)).astype(int)\n",
    "\n",
    "test_A['est'] = 1\n",
    "test_B['est'] = 1\n",
    "test_C['est'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full dataset\n",
    "\n",
    "data_A['train'] = 1\n",
    "data_B['train'] = 1\n",
    "data_C['train'] = 1\n",
    "\n",
    "test_A['train'] = 0\n",
    "test_B['train'] = 0\n",
    "test_C['train'] = 0\n",
    "\n",
    "test_A['pv_measurement'] = np.nan\n",
    "test_B['pv_measurement'] = np.nan\n",
    "test_C['pv_measurement'] = np.nan\n",
    "\n",
    "data_test_A = pd.concat([data_A, test_A], ignore_index=True)\n",
    "data_test_B = pd.concat([data_B, test_B], ignore_index=True)\n",
    "data_test_C = pd.concat([data_C, test_C], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add location\n",
    "\n",
    "data_test_A['A'] = 1\n",
    "data_test_A['B'] = 0\n",
    "data_test_A['C'] = 0\n",
    "\n",
    "data_test_B['A'] = 0\n",
    "data_test_B['B'] = 1\n",
    "data_test_B['C'] = 0\n",
    "\n",
    "data_test_C['A'] = 0\n",
    "data_test_C['B'] = 0\n",
    "data_test_C['C'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precip type to category\n",
    "categories = ['precip_none', 'precip_rain', 'precip_rain_Snow', 'precip_snow', 'precip_sleet', 'precip_freezing_rain', 'precip_hail']\n",
    "\n",
    "def add_precip_category(df, categories):\n",
    "    # Assuming 'precip_type_5min:idx' contains integer category indices\n",
    "    # Ensure the indices are integers\n",
    "    df['precip_type_5min:idx'] = df['precip_type_5min:idx'].astype(int)\n",
    "\n",
    "    # Get dummies and concatenate with the original dataframe\n",
    "    dummies = pd.get_dummies(df['precip_type_5min:idx'], prefix='precip_type').astype(int)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "data_test_A = add_precip_category(data_test_A, categories)\n",
    "data_test_B = add_precip_category(data_test_B, categories)\n",
    "data_test_C = add_precip_category(data_test_C, categories)\n",
    "\n",
    "# Drop precip_type\n",
    "data_test_A = data_test_A.drop('precip_type_5min:idx', axis='columns')\n",
    "data_test_B = data_test_B.drop('precip_type_5min:idx', axis='columns')\n",
    "data_test_C = data_test_C.drop('precip_type_5min:idx', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates columns\n",
    "\n",
    "def extract_dates(df):\n",
    "    # Convert 'date_forecast' to datetime\n",
    "    df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "\n",
    "    # Extract year, month, and day\n",
    "    df['year'] = df['date_forecast'].dt.year\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "    df['day'] = df['date_forecast'].dt.day\n",
    "\n",
    "    return df\n",
    "\n",
    "data_test_A = extract_dates(data_test_A)\n",
    "data_test_B = extract_dates(data_test_B)\n",
    "data_test_C = extract_dates(data_test_C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add daily and annual sinus curve\n",
    "\n",
    "def hour_func(x): \n",
    "    return np.cos(2*np.pi/24 * x)\n",
    "\n",
    "data_test_A[\"daily_sinus\"] = hour_func(data_test_A[\"date_forecast\"].dt.hour)\n",
    "data_test_B[\"daily_sinus\"] = hour_func(data_test_B[\"date_forecast\"].dt.hour)\n",
    "data_test_C[\"daily_sinus\"] = hour_func(data_test_C[\"date_forecast\"].dt.hour)\n",
    "\n",
    "#plot_feature(data_test_A.iloc[:40], 'daily_sinus')\n",
    "\n",
    "def day_func(x): \n",
    "    return np.cos(2*np.pi/365 * x - 2*np.pi/365 * 173)\n",
    "\n",
    "data_test_A[\"annual_sinus\"] = day_func(data_test_A[\"date_forecast\"].dt.dayofyear)\n",
    "data_test_B[\"annual_sinus\"] = day_func(data_test_B[\"date_forecast\"].dt.dayofyear)\n",
    "data_test_C[\"annual_sinus\"] = day_func(data_test_C[\"date_forecast\"].dt.dayofyear)\n",
    "\n",
    "#plot_feature(data_test_A.iloc[8000:10000], 'annual_sinus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark weird cloud data as category\n",
    "\n",
    "data_test_A['bad_cloud_data'] = (data_test_A['date_forecast'] < pd.to_datetime('2020-03-26 00:00:00')).astype(int)\n",
    "data_test_B['bad_cloud_data'] = (data_test_B['date_forecast'] < pd.to_datetime('2020-03-26 00:00:00')).astype(int)\n",
    "data_test_C['bad_cloud_data'] = (data_test_C['date_forecast'] < pd.to_datetime('2020-03-26 00:00:00')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip sun elevation. When sun elevation is below ca. -5, no pv_measurement is recorded\n",
    "\n",
    "data_test_A['sun_elevation:d'] = data_test_A['sun_elevation:d'].clip(lower=0)\n",
    "data_test_B['sun_elevation:d'] = data_test_B['sun_elevation:d'].clip(lower=0)\n",
    "data_test_C['sun_elevation:d'] = data_test_C['sun_elevation:d'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask for open sky, clip negative values for clouds\n",
    "\n",
    "data_test_A['open_sky'] = ( data_test_A['cloud_base_agl:m_y'] < 0).astype(int)\n",
    "data_test_B['open_sky'] = ( data_test_B['cloud_base_agl:m_y'] < 0).astype(int)\n",
    "data_test_C['open_sky'] = ( data_test_C['cloud_base_agl:m_y'] < 0).astype(int)\n",
    "\n",
    "data_test_A['cloud_base_agl:m_y'] = data_test_A['cloud_base_agl:m_y'].clip(lower=0)\n",
    "data_test_B['cloud_base_agl:m_y'] = data_test_B['cloud_base_agl:m_y'].clip(lower=0)\n",
    "data_test_C['cloud_base_agl:m_y'] = data_test_C['cloud_base_agl:m_y'].clip(lower=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag and lead for direct sun rad\n",
    "\n",
    "def create_lag_average_column(df, column_name, lag_window):\n",
    "    \"\"\"\n",
    "    Creates a new column in the DataFrame which is the average of the last 3 rows of the specified column.\n",
    "    For the first two rows, it uses the available values for averaging.\n",
    "\n",
    "    :param df: Pandas DataFrame.\n",
    "    :param column_name: The name of the column for which the lagged average is calculated.\n",
    "    :return: DataFrame with the new lag average column added.\n",
    "    \"\"\"\n",
    "    # Ensure that the DataFrame has the specified column\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # Calculate the rolling average of the last 3 rows\n",
    "    df[f'{column_name}_lag_avg'] = df[column_name].rolling(window=lag_window, min_periods=1).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "lag_window = 10\n",
    "\n",
    "data_test_A = create_lag_average_column(data_test_A, 'direct_rad:W', lag_window)\n",
    "data_test_B = create_lag_average_column(data_test_B, 'direct_rad:W', lag_window)\n",
    "data_test_C = create_lag_average_column(data_test_C, 'direct_rad:W', lag_window)\n",
    "\n",
    "#data_test_A = create_lag_average_column(data_test_A, 'diffuse_rad:W', lag_window)\n",
    "#data_test_B = create_lag_average_column(data_test_B, 'diffuse_rad:W', lag_window)\n",
    "#data_test_C = create_lag_average_column(data_test_C, 'diffuse_rad:W', lag_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing with lead\n",
    "\n",
    "def create_lead_average_column(df, column_name, lead_window):\n",
    "    \"\"\"\n",
    "    Creates a new column in the DataFrame which is the average of the next 3 rows of the specified column.\n",
    "    For the last two rows, it uses the available values for averaging.\n",
    "\n",
    "    :param df: Pandas DataFrame.\n",
    "    :param column_name: The name of the column for which the lead average is calculated.\n",
    "    :return: DataFrame with the new lead average column added.\n",
    "    \"\"\"\n",
    "    # Ensure that the DataFrame has the specified column\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # Reverse the DataFrame, calculate the rolling average, and then reverse it back\n",
    "    df_reversed = df.iloc[::-1].copy()\n",
    "    df_reversed[f'{column_name}_lead_avg'] = df_reversed[column_name].rolling(window=lead_window, min_periods=1).mean()\n",
    "    df[f'{column_name}_lead_avg'] = df_reversed[f'{column_name}_lead_avg'].iloc[::-1].values\n",
    "\n",
    "    return df\n",
    "\n",
    "lead_window = 1\n",
    "\n",
    "data_test_A = create_lead_average_column(data_test_A, 'direct_rad:W', lead_window)\n",
    "data_test_B = create_lead_average_column(data_test_B, 'direct_rad:W', lead_window)\n",
    "data_test_C = create_lead_average_column(data_test_C, 'direct_rad:W', lead_window)\n",
    "\n",
    "#data_test_A = create_lead_average_column(data_test_A, 'diffuse_rad:W', lead_window)\n",
    "#data_test_B = create_lead_average_column(data_test_B, 'diffuse_rad:W', lead_window)\n",
    "#data_test_C = create_lead_average_column(data_test_C, 'diffuse_rad:W', lead_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create central derivative column\n",
    "\n",
    "def create_central_difference_column(df, column_name, new_column_name):\n",
    "    \"\"\"\n",
    "    Creates a new column in the DataFrame which is the central difference of the specified column with respect\n",
    "    to the previous and next row. For the first and last rows, forward and backward differences are used.\n",
    "\n",
    "    :param df: Pandas DataFrame.\n",
    "    :param column_name: The name of the column for which the central difference is calculated.\n",
    "    :param new_column_name: The name of the new column to store the central difference.\n",
    "    :return: DataFrame with the new central difference column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that the DataFrame has the specified column\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # Calculate the central difference\n",
    "    df[new_column_name] = (df[column_name].shift(-1) - df[column_name].shift(1)) / 2\n",
    "\n",
    "    # Handle the first row using forward difference\n",
    "    df.at[0, new_column_name] = df.at[1, column_name] - df.at[0, column_name]\n",
    "\n",
    "    # Handle the last row using backward difference\n",
    "    last_idx = df.index[-1]\n",
    "    df.at[last_idx, new_column_name] = df.at[last_idx, column_name] - df.at[last_idx - 1, column_name]\n",
    "\n",
    "    return df\n",
    "\n",
    "data_test_A = create_central_difference_column(data_test_A, 'direct_rad:W', 'direct_rad_CD')\n",
    "data_test_B = create_central_difference_column(data_test_B, 'direct_rad:W', 'direct_rad_CD')\n",
    "data_test_C = create_central_difference_column(data_test_C, 'direct_rad:W', 'direct_rad_CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create difference between columns\n",
    "\n",
    "def create_difference_column(df, column1, column2, new_column_name):\n",
    "    \"\"\"\n",
    "    Creates a new column in the DataFrame which is the difference between two specified columns.\n",
    "\n",
    "    :param df: Pandas DataFrame.\n",
    "    :param column1: The name of the first column.\n",
    "    :param column2: The name of the second column.\n",
    "    :param new_column_name: The name of the new column to store the difference.\n",
    "    :return: DataFrame with the new difference column added.\n",
    "    \"\"\"\n",
    "    # Ensure that the DataFrame has the specified columns\n",
    "    if column1 not in df.columns:\n",
    "        raise ValueError(f\"Column '{column1}' not found in the DataFrame.\")\n",
    "    if column2 not in df.columns:\n",
    "        raise ValueError(f\"Column '{column2}' not found in the DataFrame.\")\n",
    "\n",
    "    # Calculate the difference and store it in the new column\n",
    "    df[new_column_name] = df[column1] - df[column2]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#data_test_A = create_difference_column(data_test_A, 'is_day:idx', 'is_in_shadow:idx', 'diff_day_shadow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "useless_col = [\n",
    "    'elevation:m',\n",
    "    'fresh_snow_12h:cm',\n",
    "    'fresh_snow_24h:cm',\n",
    "    'precip_type_2',\n",
    "    'precip_type_3',\n",
    "    'precip_type_5',\n",
    "    'precip_type_6',\n",
    "    'pressure_50m:hPa',\n",
    "    'msl_pressure:hPa',\n",
    "    'sfc_pressure:hPa',\n",
    "    'cloud_base_agl:m_x',\n",
    "]\n",
    "\n",
    "\n",
    "data_test_A = data_test_A.drop(useless_col, axis='columns')\n",
    "data_test_B = data_test_B.drop(useless_col, axis='columns')\n",
    "data_test_C = data_test_C.drop(useless_col, axis='columns')\n",
    "\n",
    "\n",
    "# C needs to drop this aswell\n",
    "data_test_C = data_test_C.drop('precip_type_4', axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset by labeled and non-labeled\n",
    "\n",
    "data_A_done = data_test_A[ data_test_A['train'] == 1 ]\n",
    "data_B_done = data_test_B[ data_test_B['train'] == 1 ]\n",
    "data_C_done = data_test_C[ data_test_C['train'] == 1 ]\n",
    "\n",
    "test_A_done = data_test_A[ data_test_A['train'] == 0 ]\n",
    "test_B_done = data_test_B[ data_test_B['train'] == 0 ]\n",
    "test_C_done = data_test_C[ data_test_C['train'] == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN pv_measurement\n",
    "\n",
    "test_A_done = test_A_done.drop('pv_measurement', axis='columns')\n",
    "test_B_done = test_B_done.drop('pv_measurement', axis='columns')\n",
    "test_C_done = test_C_done.drop('pv_measurement', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "\n",
    "data_test_A.to_csv(\"current_csv_files/data_test_A.csv\")\n",
    "data_test_B.to_csv(\"current_csv_files/data_test_B.csv\")\n",
    "data_test_C.to_csv(\"current_csv_files/data_test_C.csv\")\n",
    "\n",
    "data_A_done.to_csv(\"current_csv_files/data_A.csv\")\n",
    "data_B_done.to_csv(\"current_csv_files/data_B.csv\")\n",
    "data_C_done.to_csv(\"current_csv_files/data_C.csv\")\n",
    "\n",
    "test_A_done.to_csv(\"current_csv_files/test_A.csv\")\n",
    "test_C_done.to_csv(\"current_csv_files/test_C.csv\")\n",
    "test_B_done.to_csv(\"current_csv_files/test_B.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
