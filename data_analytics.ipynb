{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: .\n",
      "  File: .DS_Store\n",
      "  File: .gitignore\n",
      "  File: catboost_model.ipynb\n",
      "  File: data_analytics.ipynb\n",
      "  File: data_preparation.ipynb\n",
      "  File: feature_engineering.ipynb\n",
      "  File: helpers.py\n",
      "  File: predictions.ipynb\n",
      "  File: README.md\n",
      "  File: Readme1.md\n",
      "  File: read_files.ipynb\n",
      "  File: stacking_xgb_cat_ada.ipynb\n",
      "  File: teodor_pred2.ipynb\n",
      "  File: teodor_pred3.ipynb\n",
      "  File: teodor_predictions.ipynb\n",
      "Directory: .\\.git\n",
      "  File: config\n",
      "  File: description\n",
      "  File: HEAD\n",
      "  File: index\n",
      "  File: packed-refs\n",
      "Directory: .\\.git\\hooks\n",
      "  File: applypatch-msg.sample\n",
      "  File: commit-msg.sample\n",
      "  File: post-update.sample\n",
      "  File: pre-applypatch.sample\n",
      "  File: pre-commit.sample\n",
      "  File: pre-push.sample\n",
      "  File: pre-rebase.sample\n",
      "  File: pre-receive.sample\n",
      "  File: prepare-commit-msg.sample\n",
      "  File: update.sample\n",
      "Directory: .\\.git\\info\n",
      "  File: exclude\n",
      "Directory: .\\.git\\logs\n",
      "  File: HEAD\n",
      "Directory: .\\.git\\logs\\refs\n",
      "Directory: .\\.git\\logs\\refs\\heads\n",
      "  File: main\n",
      "Directory: .\\.git\\logs\\refs\\remotes\n",
      "Directory: .\\.git\\logs\\refs\\remotes\\origin\n",
      "  File: HEAD\n",
      "Directory: .\\.git\\objects\n",
      "Directory: .\\.git\\objects\\info\n",
      "Directory: .\\.git\\objects\\pack\n",
      "  File: pack-2f1bd1adcb8f667fea13303b626c20384c3b2498.idx\n",
      "  File: pack-2f1bd1adcb8f667fea13303b626c20384c3b2498.pack\n",
      "Directory: .\\.git\\refs\n",
      "Directory: .\\.git\\refs\\heads\n",
      "  File: main\n",
      "Directory: .\\.git\\refs\\remotes\n",
      "Directory: .\\.git\\refs\\remotes\\origin\n",
      "  File: HEAD\n",
      "Directory: .\\.git\\refs\\tags\n",
      "Directory: .\\A\n",
      "  File: train_targets.parquet\n",
      "  File: X_test_estimated.parquet\n",
      "  File: X_train_estimated.parquet\n",
      "  File: X_train_observed.parquet\n",
      "Directory: .\\B\n",
      "  File: train_targets.parquet\n",
      "  File: X_test_estimated.parquet\n",
      "  File: X_train_estimated.parquet\n",
      "  File: X_train_observed.parquet\n",
      "Directory: .\\C\n",
      "  File: train_targets.parquet\n",
      "  File: X_test_estimated.parquet\n",
      "  File: X_train_estimated.parquet\n",
      "  File: X_train_observed.parquet\n",
      "Directory: .\\catboost_info\n",
      "  File: catboost_training.json\n",
      "  File: learn_error.tsv\n",
      "  File: test_error.tsv\n",
      "  File: time_left.tsv\n",
      "Directory: .\\catboost_info\\learn\n",
      "  File: events.out.tfevents\n",
      "Directory: .\\catboost_info\\test\n",
      "  File: events.out.tfevents\n",
      "Directory: .\\data_prep\n",
      "  File: data_manager.py\n",
      "Directory: .\\data_prep\\__pycache__\n",
      "  File: data_manager.cpython-310.pyc\n",
      "Directory: .\\log_preds\n",
      "  File: logpred1.ipynb\n",
      "  File: log_preds.ipynb\n",
      "Directory: .\\subs\n",
      "  File: .DS_Store\n",
      "Directory: .\\__pycache__\n",
      "  File: data_manager.cpython-310.pyc\n",
      "  File: helpers.cpython-310.pyc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "def list_directory_tree_with_os_walk(starting_directory):\n",
    "    for root, directories, files in os.walk(starting_directory):\n",
    "        print(f\"Directory: {root}\")\n",
    "        for file in files:\n",
    "            print(f\"  File: {file}\")\n",
    "\n",
    "list_directory_tree_with_os_walk('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting NULL values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in A: \n",
      "Count estimate train ----> \n",
      "ceiling_height_agl:m 3919\n",
      "cloud_base_agl:m 2094\n",
      "snow_density:kgm3 15769\n",
      "--------------------------------------------------------------------------------\n",
      "Count observed train ----> \n",
      "ceiling_height_agl:m 22247\n",
      "cloud_base_agl:m 8066\n",
      "snow_density:kgm3 115945\n",
      "--------------------------------------------------------------------------------\n",
      "Count estimate test ----> \n",
      "ceiling_height_agl:m 793\n",
      "cloud_base_agl:m 298\n",
      "snow_density:kgm3 2880\n",
      "--------------------------------------------------------------------------------\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "Null values in B: \n",
      "Count estimate train ----> \n",
      "ceiling_height_agl:m 3689\n",
      "cloud_base_agl:m 1963\n",
      "snow_density:kgm3 15713\n",
      "--------------------------------------------------------------------------------\n",
      "Count observed train ----> \n",
      "ceiling_height_agl:m 18772\n",
      "cloud_base_agl:m 7473\n",
      "snow_density:kgm3 111201\n",
      "--------------------------------------------------------------------------------\n",
      "Count estimate test ----> \n",
      "ceiling_height_agl:m 755\n",
      "cloud_base_agl:m 277\n",
      "snow_density:kgm3 2880\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Counting NULL values in location a\n",
    "\n",
    "null_count_estimated_a = X_train_estimated_a.isnull().sum()\n",
    "null_count_observed_a = X_train_observed_a.isnull().sum()\n",
    "null_count_estimated_test_a = X_test_estimated_a.isnull().sum()\n",
    "\n",
    "# Counting NULL values in location b\n",
    "\n",
    "null_count_estimated_b = X_train_estimated_b.isnull().sum()\n",
    "null_count_observed_b = X_train_observed_b.isnull().sum()\n",
    "null_count_estimated_test_b = X_test_estimated_b.isnull().sum()\n",
    "\n",
    "# Counting NULL values in location c\n",
    "\n",
    "null_count_estimated_c = X_train_estimated_c.isnull().sum()\n",
    "null_count_observed_c = X_train_observed_c.isnull().sum()\n",
    "null_count_estimated_test_c = X_test_estimated_c.isnull().sum()\n",
    "\n",
    "\n",
    "print(\"Null values in A: \")\n",
    "print(\"Count estimate train ----> \" , )\n",
    "for item in null_count_estimated_a.to_dict(): \n",
    "    if null_count_estimated_a.to_dict()[item] != 0: \n",
    "        print(item, null_count_estimated_a.to_dict()[item])\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Count observed train ----> \" )\n",
    "for item in null_count_observed_a.to_dict(): \n",
    "    if null_count_observed_a.to_dict()[item] != 0: \n",
    "        print(item, null_count_observed_a.to_dict()[item])\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Count estimate test ----> \"  )\n",
    "for item in null_count_estimated_test_a.to_dict(): \n",
    "    if null_count_estimated_test_a.to_dict()[item] != 0: \n",
    "        print(item, null_count_estimated_test_a.to_dict()[item])\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "print(\"\")\n",
    "print(\"Null values in B: \")\n",
    "print(\"Count estimate train ----> \" , )\n",
    "for item in null_count_estimated_b.to_dict(): \n",
    "    if null_count_estimated_b.to_dict()[item] != 0: \n",
    "        print(item, null_count_estimated_b.to_dict()[item])\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Count observed train ----> \" )\n",
    "for item in null_count_observed_b.to_dict(): \n",
    "    if null_count_observed_b.to_dict()[item] != 0: \n",
    "        print(item, null_count_observed_b.to_dict()[item])\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Count estimate test ----> \"  )\n",
    "for item in null_count_estimated_test_b.to_dict(): \n",
    "    if null_count_estimated_test_b.to_dict()[item] != 0: \n",
    "        print(item, null_count_estimated_test_b.to_dict()[item])\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove the null value dominating columns\n",
    "\n",
    "X_train_estimated_a = X_train_estimated_a.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_train_observed_a = X_train_observed_a.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_test_estimated_a = X_test_estimated_a.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_test_estimated_a = X_test_estimated_a.drop(columns=\"date_calc\", axis=1)\n",
    "\n",
    "X_train_estimated_a = X_train_estimated_a.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "X_train_observed_a = X_train_observed_a.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "X_test_estimated_a = X_test_estimated_a.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "\n",
    "X_train_estimated_b = X_train_estimated_b.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_train_observed_b = X_train_observed_b.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_test_estimated_b = X_test_estimated_b.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_test_estimated_b = X_test_estimated_b.drop(columns=\"date_calc\", axis=1)\n",
    "\n",
    "X_train_estimated_b = X_train_estimated_b.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "X_train_observed_b = X_train_observed_b.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "X_test_estimated_b = X_test_estimated_b.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "\n",
    "X_train_estimated_c = X_train_estimated_c.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_train_observed_c = X_train_observed_c.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_test_estimated_c = X_test_estimated_c.drop(columns=\"snow_density:kgm3\", axis=1)\n",
    "X_test_estimated_c = X_test_estimated_c.drop(columns=\"date_calc\", axis=1)\n",
    "\n",
    "X_train_estimated_c = X_train_estimated_c.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "X_train_observed_c = X_train_observed_c.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "X_test_estimated_c = X_test_estimated_c.drop(columns=\"cloud_base_agl:m\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at data Head and Tail \n",
    "\n",
    "Datasets \"_estimated_\" and \"_observed_\" are sampled every 15 minutes, whilst \"_test_\" are sampled at hourly intervals. \n",
    "\n",
    "Therefore, we average the observations each hour to sample at the same intervals as the estimations.\n",
    "\n",
    "- in the test dataset, we are given 30 days scattered between 05/01 untill 07/03. Using reshape fills in the missing dates so we have to remove the added dates from our test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_test_estimated_a_corr' (DataFrame)\n",
      "Stored 'X_test_estimated_b_corr' (DataFrame)\n",
      "Stored 'X_test_estimated_c_corr' (DataFrame)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "absolute_humidity_2m:gm3          0\n",
       "air_density_2m:kgm3               0\n",
       "ceiling_height_agl:m              0\n",
       "clear_sky_energy_1h:J             0\n",
       "clear_sky_rad:W                   0\n",
       "dew_or_rime:idx                   0\n",
       "dew_point_2m:K                    0\n",
       "diffuse_rad:W                     0\n",
       "diffuse_rad_1h:J                  0\n",
       "direct_rad:W                      0\n",
       "direct_rad_1h:J                   0\n",
       "effective_cloud_cover:p           0\n",
       "elevation:m                       0\n",
       "fresh_snow_12h:cm                 0\n",
       "fresh_snow_1h:cm                  0\n",
       "fresh_snow_24h:cm                 0\n",
       "fresh_snow_3h:cm                  0\n",
       "fresh_snow_6h:cm                  0\n",
       "is_day:idx                        0\n",
       "is_in_shadow:idx                  0\n",
       "msl_pressure:hPa                  0\n",
       "precip_5min:mm                    0\n",
       "precip_type_5min:idx              0\n",
       "pressure_100m:hPa                 0\n",
       "pressure_50m:hPa                  0\n",
       "prob_rime:p                       0\n",
       "rain_water:kgm2                   0\n",
       "relative_humidity_1000hPa:p       0\n",
       "sfc_pressure:hPa                  0\n",
       "snow_depth:cm                     0\n",
       "snow_drift:idx                    0\n",
       "snow_melt_10min:mm                0\n",
       "snow_water:kgm2                   0\n",
       "sun_azimuth:d                     0\n",
       "sun_elevation:d                   0\n",
       "super_cooled_liquid_water:kgm2    0\n",
       "t_1000hPa:K                       0\n",
       "total_cloud_cover:p               0\n",
       "visibility:m                      0\n",
       "wind_speed_10m:ms                 0\n",
       "wind_speed_u_10m:ms               0\n",
       "wind_speed_v_10m:ms               0\n",
       "wind_speed_w_1000hPa:ms           0\n",
       "date_forecast                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "#X_train_estimated_a.head() # 15 min \n",
    "#X_test_estimated_a.head() # 15 min\n",
    "#X_train_observed_a.head() # 15 min \n",
    "#train_a.head() # hourly \n",
    "\n",
    "### fixing a\n",
    "\n",
    "# We impute to fill in the missing values, this can also be done with regression or more complex imputers\n",
    "\n",
    "imputed = helpers.impute([X_train_observed_a, X_train_estimated_a, X_test_estimated_a])\n",
    "\n",
    "X_train_observed_a = imputed[0]\n",
    "X_train_estimated_a = imputed[1]\n",
    "X_test_estimated_a = imputed[2]\n",
    "\n",
    "# averaging so we only have every hour / however this adds dates missing, we have 30 days scattered, this gives us all 64 days between 05-01 to 07-03\n",
    "X_train_observed_a_hourly = X_train_observed_a.resample(\"H\", on=\"date_forecast\").mean()\n",
    "X_train_estimated_a_hourly = X_train_estimated_a.resample(\"H\", on=\"date_forecast\").mean()\n",
    "X_test_estimated_a_hourly = X_test_estimated_a.resample(\"H\", on=\"date_forecast\").mean()\n",
    "\n",
    "## to fix this we store all the unique dates from the original set to compare later \n",
    "dat_list_train_observed = X_train_observed_a[\"date_forecast\"].dt.date.unique().tolist()\n",
    "dat_list_train_estimated = X_train_estimated_a[\"date_forecast\"].dt.date.unique().tolist()\n",
    "dat_list_test_estimated = X_test_estimated_a[\"date_forecast\"].dt.date.unique().tolist()\n",
    "\n",
    "## we need the date forecast back as feature, as the resample changes the index for some reason\n",
    "X_train_observed_a_hourly[\"date_forecast\"] = X_train_observed_a_hourly.index\n",
    "X_train_estimated_a_hourly[\"date_forecast\"] = X_train_estimated_a_hourly.index\n",
    "X_test_estimated_a_hourly[\"date_forecast\"] = X_test_estimated_a_hourly.index\n",
    "\n",
    "## returns only the correct dates! \n",
    "X_train_observed_a_corr = X_train_observed_a_hourly[X_train_observed_a_hourly[\"date_forecast\"].dt.date.isin(dat_list_train_observed)]\n",
    "X_train_estimated_a_corr = X_train_estimated_a_hourly[X_train_estimated_a_hourly[\"date_forecast\"].dt.date.isin(dat_list_train_estimated)]\n",
    "X_test_estimated_a_corr = X_test_estimated_a_hourly[X_test_estimated_a_hourly[\"date_forecast\"].dt.date.isin(dat_list_test_estimated)]\n",
    "\n",
    "\n",
    "# ## fixing b\n",
    "\n",
    "imputed = helpers.impute([X_train_observed_b, X_train_estimated_b, X_test_estimated_b])\n",
    "\n",
    "X_train_observed_b = imputed[0]\n",
    "X_train_estimated_b = imputed[1]\n",
    "X_test_estimated_b = imputed[2]\n",
    "\n",
    "X_train_observed_b_hourly = X_train_observed_b.resample(\"H\", on=\"date_forecast\").mean()\n",
    "X_train_estimated_b_hourly = X_train_estimated_b.resample(\"H\", on=\"date_forecast\").mean()\n",
    "X_test_estimated_b_hourly = X_test_estimated_b.resample(\"H\", on=\"date_forecast\").mean()\n",
    "\n",
    "dat_list_train_observed = X_train_observed_b[\"date_forecast\"].dt.date.unique().tolist()\n",
    "dat_list_train_estimated = X_train_estimated_b[\"date_forecast\"].dt.date.unique().tolist()\n",
    "dat_list_test_estimated = X_test_estimated_b[\"date_forecast\"].dt.date.unique().tolist()\n",
    "\n",
    "X_train_observed_b_hourly[\"date_forecast\"] = X_train_observed_b_hourly.index\n",
    "X_train_estimated_b_hourly[\"date_forecast\"] = X_train_estimated_b_hourly.index\n",
    "X_test_estimated_b_hourly[\"date_forecast\"] = X_test_estimated_b_hourly.index\n",
    "\n",
    "X_train_observed_b_corr = X_train_observed_b_hourly[X_train_observed_b_hourly[\"date_forecast\"].dt.date.isin(dat_list_train_observed)]\n",
    "X_train_estimated_b_corr = X_train_estimated_b_hourly[X_train_estimated_b_hourly[\"date_forecast\"].dt.date.isin(dat_list_train_estimated)]\n",
    "X_test_estimated_b_corr = X_test_estimated_b_hourly[X_test_estimated_b_hourly[\"date_forecast\"].dt.date.isin(dat_list_test_estimated)]\n",
    "\n",
    "# ## fixing c\n",
    "imputed = helpers.impute([X_train_observed_c, X_train_estimated_c, X_test_estimated_c])\n",
    "\n",
    "X_train_observed_c = imputed[0]\n",
    "X_train_estimated_c = imputed[1]\n",
    "X_test_estimated_c = imputed[2]\n",
    "\n",
    "X_train_observed_c_hourly = X_train_observed_c.resample(\"H\", on=\"date_forecast\").mean()\n",
    "X_train_estimated_c_hourly = X_train_estimated_c.resample(\"H\", on=\"date_forecast\").mean()\n",
    "X_test_estimated_c_hourly = X_test_estimated_c.resample(\"H\", on=\"date_forecast\").mean()\n",
    "\n",
    "dat_list_train_observed = X_train_observed_c[\"date_forecast\"].dt.date.unique().tolist()\n",
    "dat_list_train_estimated = X_train_estimated_c[\"date_forecast\"].dt.date.unique().tolist()\n",
    "dat_list_test_estimated = X_test_estimated_c[\"date_forecast\"].dt.date.unique().tolist()\n",
    "\n",
    "X_train_observed_c_hourly[\"date_forecast\"] = X_train_observed_c_hourly.index\n",
    "X_train_estimated_c_hourly[\"date_forecast\"] = X_train_estimated_c_hourly.index\n",
    "X_test_estimated_c_hourly[\"date_forecast\"] = X_test_estimated_c_hourly.index\n",
    "\n",
    "X_train_observed_c_corr = X_train_observed_c_hourly[X_train_observed_c_hourly[\"date_forecast\"].dt.date.isin(dat_list_train_observed)]\n",
    "X_train_estimated_c_corr = X_train_estimated_c_hourly[X_train_estimated_c_hourly[\"date_forecast\"].dt.date.isin(dat_list_train_estimated)]\n",
    "X_test_estimated_c_corr = X_test_estimated_c_hourly[X_test_estimated_c_hourly[\"date_forecast\"].dt.date.isin(dat_list_test_estimated)]\n",
    "\n",
    "# storing data for use in other notebooks\n",
    "%store X_test_estimated_a_corr\n",
    "%store X_test_estimated_b_corr\n",
    "%store X_test_estimated_c_corr\n",
    "\n",
    "\n",
    "X_train_estimated_c_corr.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Graphing the importance of different features according to some common regressor algorithms \n",
    "\n",
    "Noticed that \"direct rad\" features dominated, removing these gave more insight into the other feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a.columns = [\"date_forecast\", \"pv_measurement\"]\n",
    "train_a.set_index(\"date_forecast\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_forecast                     0\n",
      "pv_measurement                    0\n",
      "absolute_humidity_2m:gm3          0\n",
      "air_density_2m:kgm3               0\n",
      "ceiling_height_agl:m              0\n",
      "clear_sky_energy_1h:J             0\n",
      "clear_sky_rad:W                   0\n",
      "dew_or_rime:idx                   0\n",
      "dew_point_2m:K                    0\n",
      "diffuse_rad:W                     0\n",
      "diffuse_rad_1h:J                  0\n",
      "direct_rad:W                      0\n",
      "direct_rad_1h:J                   0\n",
      "effective_cloud_cover:p           0\n",
      "elevation:m                       0\n",
      "fresh_snow_12h:cm                 0\n",
      "fresh_snow_1h:cm                  0\n",
      "fresh_snow_24h:cm                 0\n",
      "fresh_snow_3h:cm                  0\n",
      "fresh_snow_6h:cm                  0\n",
      "is_day:idx                        0\n",
      "is_in_shadow:idx                  0\n",
      "msl_pressure:hPa                  0\n",
      "precip_5min:mm                    0\n",
      "precip_type_5min:idx              0\n",
      "pressure_100m:hPa                 0\n",
      "pressure_50m:hPa                  0\n",
      "prob_rime:p                       0\n",
      "rain_water:kgm2                   0\n",
      "relative_humidity_1000hPa:p       0\n",
      "sfc_pressure:hPa                  0\n",
      "snow_depth:cm                     0\n",
      "snow_drift:idx                    0\n",
      "snow_melt_10min:mm                0\n",
      "snow_water:kgm2                   0\n",
      "sun_azimuth:d                     0\n",
      "sun_elevation:d                   0\n",
      "super_cooled_liquid_water:kgm2    0\n",
      "t_1000hPa:K                       0\n",
      "total_cloud_cover:p               0\n",
      "visibility:m                      0\n",
      "wind_speed_10m:ms                 0\n",
      "wind_speed_u_10m:ms               0\n",
      "wind_speed_v_10m:ms               0\n",
      "wind_speed_w_1000hPa:ms           0\n",
      "dtype: int64\n",
      "Stored 'data_A' (DataFrame)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henrilja\\AppData\\Local\\Temp\\ipykernel_15568\\693499011.py:5: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_A = data_A.interpolate(\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data_A = pd.concat([X_train_observed_a_hourly, X_train_estimated_a_hourly], join=\"outer\", ignore_index=True) \n",
    "\n",
    "data_A = pd.merge(train_a, train_data_A, how=\"left\", on=\"date_forecast\") \n",
    "\n",
    "data_A = data_A.interpolate(\"bfill\")\n",
    "#data_A = data_A.fillna(0.0)\n",
    "\n",
    "print(data_A.isna().sum())\n",
    "\n",
    "cols = data_A.columns.tolist()\n",
    "cols[0] = \"pv_measurement\"\n",
    "cols[1] = \"date_forecast\"\n",
    "\n",
    "data_A = data_A[cols]\n",
    "\n",
    "\n",
    "\n",
    "%store data_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_A' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store data_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b.columns = [\"date_forecast\", \"pv_measurement\"]\n",
    "train_b.set_index(\"date_forecast\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_forecast                     0\n",
      "pv_measurement                    0\n",
      "absolute_humidity_2m:gm3          0\n",
      "air_density_2m:kgm3               0\n",
      "ceiling_height_agl:m              0\n",
      "clear_sky_energy_1h:J             0\n",
      "clear_sky_rad:W                   0\n",
      "dew_or_rime:idx                   0\n",
      "dew_point_2m:K                    0\n",
      "diffuse_rad:W                     0\n",
      "diffuse_rad_1h:J                  0\n",
      "direct_rad:W                      0\n",
      "direct_rad_1h:J                   0\n",
      "effective_cloud_cover:p           0\n",
      "elevation:m                       0\n",
      "fresh_snow_12h:cm                 0\n",
      "fresh_snow_1h:cm                  0\n",
      "fresh_snow_24h:cm                 0\n",
      "fresh_snow_3h:cm                  0\n",
      "fresh_snow_6h:cm                  0\n",
      "is_day:idx                        0\n",
      "is_in_shadow:idx                  0\n",
      "msl_pressure:hPa                  0\n",
      "precip_5min:mm                    0\n",
      "precip_type_5min:idx              0\n",
      "pressure_100m:hPa                 0\n",
      "pressure_50m:hPa                  0\n",
      "prob_rime:p                       0\n",
      "rain_water:kgm2                   0\n",
      "relative_humidity_1000hPa:p       0\n",
      "sfc_pressure:hPa                  0\n",
      "snow_depth:cm                     0\n",
      "snow_drift:idx                    0\n",
      "snow_melt_10min:mm                0\n",
      "snow_water:kgm2                   0\n",
      "sun_azimuth:d                     0\n",
      "sun_elevation:d                   0\n",
      "super_cooled_liquid_water:kgm2    0\n",
      "t_1000hPa:K                       0\n",
      "total_cloud_cover:p               0\n",
      "visibility:m                      0\n",
      "wind_speed_10m:ms                 0\n",
      "wind_speed_u_10m:ms               0\n",
      "wind_speed_v_10m:ms               0\n",
      "wind_speed_w_1000hPa:ms           0\n",
      "dtype: int64\n",
      "Stored 'data_B' (DataFrame)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henrilja\\AppData\\Local\\Temp\\ipykernel_15568\\747070351.py:5: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_B = data_B.interpolate(\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_B = pd.concat([X_train_observed_b_hourly, X_train_estimated_b_hourly], join=\"outer\", ignore_index=True) \n",
    "\n",
    "data_B = pd.merge(train_b, train_data_B, how=\"left\", on=\"date_forecast\") \n",
    "\n",
    "data_B = data_B.interpolate(\"bfill\")\n",
    "#data_A = data_A.fillna(0.0)\n",
    "\n",
    "print(data_B.isna().sum())\n",
    "\n",
    "cols = data_B.columns.tolist()\n",
    "cols[0] = \"pv_measurement\"\n",
    "cols[1] = \"date_forecast\"\n",
    "\n",
    "data_B = data_B[cols]\n",
    "\n",
    "\n",
    "\n",
    "%store data_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c.columns = [\"date_forecast\", \"pv_measurement\"]\n",
    "train_c.set_index(\"date_forecast\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_forecast                     0\n",
      "pv_measurement                    0\n",
      "absolute_humidity_2m:gm3          0\n",
      "air_density_2m:kgm3               0\n",
      "ceiling_height_agl:m              0\n",
      "clear_sky_energy_1h:J             0\n",
      "clear_sky_rad:W                   0\n",
      "dew_or_rime:idx                   0\n",
      "dew_point_2m:K                    0\n",
      "diffuse_rad:W                     0\n",
      "diffuse_rad_1h:J                  0\n",
      "direct_rad:W                      0\n",
      "direct_rad_1h:J                   0\n",
      "effective_cloud_cover:p           0\n",
      "elevation:m                       0\n",
      "fresh_snow_12h:cm                 0\n",
      "fresh_snow_1h:cm                  0\n",
      "fresh_snow_24h:cm                 0\n",
      "fresh_snow_3h:cm                  0\n",
      "fresh_snow_6h:cm                  0\n",
      "is_day:idx                        0\n",
      "is_in_shadow:idx                  0\n",
      "msl_pressure:hPa                  0\n",
      "precip_5min:mm                    0\n",
      "precip_type_5min:idx              0\n",
      "pressure_100m:hPa                 0\n",
      "pressure_50m:hPa                  0\n",
      "prob_rime:p                       0\n",
      "rain_water:kgm2                   0\n",
      "relative_humidity_1000hPa:p       0\n",
      "sfc_pressure:hPa                  0\n",
      "snow_depth:cm                     0\n",
      "snow_drift:idx                    0\n",
      "snow_melt_10min:mm                0\n",
      "snow_water:kgm2                   0\n",
      "sun_azimuth:d                     0\n",
      "sun_elevation:d                   0\n",
      "super_cooled_liquid_water:kgm2    0\n",
      "t_1000hPa:K                       0\n",
      "total_cloud_cover:p               0\n",
      "visibility:m                      0\n",
      "wind_speed_10m:ms                 0\n",
      "wind_speed_u_10m:ms               0\n",
      "wind_speed_v_10m:ms               0\n",
      "wind_speed_w_1000hPa:ms           0\n",
      "dtype: int64\n",
      "Stored 'data_C' (DataFrame)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henrilja\\AppData\\Local\\Temp\\ipykernel_15568\\2574858493.py:5: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_C = data_C.interpolate(\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_C = pd.concat([X_train_observed_c_hourly, X_train_estimated_c_hourly], join=\"outer\", ignore_index=True) \n",
    "\n",
    "data_C = pd.merge(train_c, train_data_C, how=\"left\", on=\"date_forecast\") \n",
    "\n",
    "data_C = data_C.interpolate(\"bfill\")\n",
    "#data_A = data_A.fillna(0.0)\n",
    "\n",
    "print(data_C.isna().sum())\n",
    "\n",
    "cols = data_C.columns.tolist()\n",
    "cols[0] = \"pv_measurement\"\n",
    "cols[1] = \"date_forecast\"\n",
    "\n",
    "data_C = data_C[cols]\n",
    "\n",
    "\n",
    "\n",
    "%store data_C\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
